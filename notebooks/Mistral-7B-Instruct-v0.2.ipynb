{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# å®‰è£…å¿…è¦åº“\n",
    "!pip install transformers accelerate bitsandbytes gradio\n",
    "\n",
    "# ç™»å½• Hugging Faceï¼ˆå¦‚æœä½ è¿˜æ²¡ç™»å½•ï¼‰\n",
    "from huggingface_hub import login\n",
    "login(new_session=False)\n",
    "\n",
    "# åŠ è½½æ¨¡å‹å’Œ tokenizerï¼ˆä½¿ç”¨ 4-bit é‡åŒ–ä»¥èŠ‚çœæ˜¾å­˜ï¼‰\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "import torch\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# è®¾ç½®é‡åŒ–é…ç½®ä»¥æå‡æ¨ç†é€Ÿåº¦\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "# åŠ è½½ tokenizer å’Œæ¨¡å‹\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # é¿å… pad_token è­¦å‘Š\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    "    use_auth_token=True\n",
    ")\n",
    "\n",
    "# åˆ›å»ºå¯¹è¯ pipeline\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# å®šä¹‰å¯¹è¯å‡½æ•°ï¼ˆæ”¯æŒå¤šè½®å¯¹è¯ï¼‰\n",
    "def chat_interface(user_input, history):\n",
    "    # æ„å»º promptï¼ˆåŒ…å«å†å²ï¼‰\n",
    "    prompt = \"<s>\"\n",
    "    for i in range(0, len(history), 2):\n",
    "        user_msg = history[i][\"content\"]\n",
    "        assistant_msg = history[i+1][\"content\"] if i+1 < len(history) else \"\"\n",
    "        prompt += f\"[INST] {user_msg} [/INST] {assistant_msg} \"\n",
    "    prompt += f\"[INST] {user_input} [/INST]\"\n",
    "\n",
    "    # æ¨¡å‹ç”Ÿæˆ\n",
    "    output = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7)[0]['generated_text']\n",
    "    response = output.split(\"[/INST]\")[-1].strip()\n",
    "\n",
    "    # æ›´æ–°å†å²\n",
    "    history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    history.append({\"role\": \"assistant\", \"content\": response})\n",
    "    return history, history\n",
    "\n",
    "# åˆ›å»º Gradio ç•Œé¢\n",
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## ğŸ¤– Mistral-7B å¯¹è¯æœºå™¨äºº\")\n",
    "    chatbot = gr.Chatbot(label=\"å¯¹è¯çª—å£\", type=\"messages\")\n",
    "    msg = gr.Textbox(label=\"è¯·è¾“å…¥ä½ çš„é—®é¢˜\")\n",
    "    clear = gr.Button(\"æ¸…é™¤å¯¹è¯\")\n",
    "    \n",
    "    state = gr.State([])\n",
    "\n",
    "    msg.submit(chat_interface, [msg, state], [chatbot, state])\n",
    "    clear.click(lambda: ([], []), None, [chatbot, state])\n",
    "\n",
    "# å¯åŠ¨ç•Œé¢\n",
    "demo.launch(share=True, debug=True)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
