{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 安装必要库\n",
    "!pip install transformers accelerate bitsandbytes gradio\n",
    "\n",
    "# 登录 Hugging Face（如果你还没登录）\n",
    "from huggingface_hub import login\n",
    "login(new_session=False)\n",
    "\n",
    "# 加载模型和 tokenizer（使用 4-bit 量化以节省显存）\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "import torch\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# 设置量化配置以提升推理速度\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "# 加载 tokenizer 和模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # 避免 pad_token 警告\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    "    use_auth_token=True\n",
    ")\n",
    "\n",
    "# 创建对话 pipeline\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# 定义对话函数（支持多轮对话）\n",
    "def chat_interface(user_input, history):\n",
    "    # 构建 prompt（包含历史）\n",
    "    prompt = \"<s>\"\n",
    "    for i in range(0, len(history), 2):\n",
    "        user_msg = history[i][\"content\"]\n",
    "        assistant_msg = history[i+1][\"content\"] if i+1 < len(history) else \"\"\n",
    "        prompt += f\"[INST] {user_msg} [/INST] {assistant_msg} \"\n",
    "    prompt += f\"[INST] {user_input} [/INST]\"\n",
    "\n",
    "    # 模型生成\n",
    "    output = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7)[0]['generated_text']\n",
    "    response = output.split(\"[/INST]\")[-1].strip()\n",
    "\n",
    "    # 更新历史\n",
    "    history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    history.append({\"role\": \"assistant\", \"content\": response})\n",
    "    return history, history\n",
    "\n",
    "# 创建 Gradio 界面\n",
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## 🤖 Mistral-7B 对话机器人\")\n",
    "    chatbot = gr.Chatbot(label=\"对话窗口\", type=\"messages\")\n",
    "    msg = gr.Textbox(label=\"请输入你的问题\")\n",
    "    clear = gr.Button(\"清除对话\")\n",
    "    \n",
    "    state = gr.State([])\n",
    "\n",
    "    msg.submit(chat_interface, [msg, state], [chatbot, state])\n",
    "    clear.click(lambda: ([], []), None, [chatbot, state])\n",
    "\n",
    "# 启动界面\n",
    "demo.launch(share=True, debug=True)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
