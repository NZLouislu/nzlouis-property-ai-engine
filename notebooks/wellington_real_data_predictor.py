#!/usr/bin/env python3
"""
Wellingtonæˆ¿äº§é¢„æµ‹æ¨¡å‹ - åŸºäºçœŸå®æ•°æ®
ä»real_estateè¡¨è·å–è®­ç»ƒæ•°æ®ï¼Œå¯¹propertiesè¡¨ä¸­çš„Wellingtonæˆ¿äº§è¿›è¡Œé¢„æµ‹
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, accuracy_score
from sklearn.linear_model import LogisticRegression
import joblib
from datetime import datetime
import warnings
import matplotlib.pyplot as plt
import seaborn as sns
import os
import sys
import json
from dotenv import load_dotenv
from supabase import create_client, Client
import uuid

warnings.filterwarnings("ignore")

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

def create_supabase_client() -> Client:
    """åˆ›å»ºSupabaseå®¢æˆ·ç«¯"""
    try:
        url = os.getenv("SUPABASE_URL")
        key = os.getenv("SUPABASE_KEY")

        if not url or not key:
            raise ValueError("SUPABASE_URLå’ŒSUPABASE_KEYç¯å¢ƒå˜é‡å¿…é¡»è®¾ç½®")

        return create_client(url, key)
    except Exception as e:
        print(f"âŒ åˆ›å»ºSupabaseå®¢æˆ·ç«¯å¤±è´¥: {e}")
        return None

def get_training_data_from_real_estate(supabase_client):
    """ä»real_estateè¡¨è·å–Wellingtonå’ŒAucklandçš„é”€å”®æ•°æ®ä½œä¸ºè®­ç»ƒé›†"""
    if not supabase_client:
        print("âŒ æ•°æ®åº“è¿æ¥ä¸å¯ç”¨ï¼Œæ— æ³•è·å–è®­ç»ƒæ•°æ®")
        return None
    
    print("ğŸ”„ ä»real_estateè¡¨è·å–è®­ç»ƒæ•°æ®...")
    
    try:
        # è·å–æ‰€æœ‰æ•°æ®
        response = supabase_client.table('real_estate').select('*').execute()
        
        if not response.data:
            print("âš ï¸ real_estateè¡¨ä¸­æ²¡æœ‰æ•°æ®")
            return None
            
        df = pd.DataFrame(response.data)
        print(f"âœ… ä»real_estateè¡¨è·å–äº† {len(df)} æ¡è®°å½•")
        print(f"ğŸ“‹ è·å–çš„åˆ—: {list(df.columns)}")
        
        # æ˜¾ç¤ºå‰å‡ æ¡æ•°æ®ä»¥äº†è§£ç»“æ„
        print("\nğŸ“Š æ•°æ®æ ·æœ¬:")
        print(df.head())
        
        return df
        
    except Exception as e:
        print(f"âŒ è·å–è®­ç»ƒæ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return None

def process_real_estate_data(df):
    """å¤„ç†real_estateè¡¨çš„æ•°æ®ï¼Œæå–ç‰¹å¾"""
    if df is None or len(df) == 0:
        return None, None
    
    print("ğŸ”„ å¤„ç†real_estateæ•°æ®...")
    
    processed_data = df.copy()
    
    # è§£ædataå­—æ®µä¸­çš„JSONæ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if 'data' in processed_data.columns:
        try:
            data_features = []
            
            for idx, row in processed_data.iterrows():
                try:
                    if pd.notna(row['data']) and row['data']:
                        if isinstance(row['data'], str):
                            data_dict = json.loads(row['data'])
                        else:
                            data_dict = row['data']
                        data_features.append(data_dict)
                    else:
                        data_features.append({})
                except:
                    data_features.append({})
            
            # å°†JSONæ•°æ®è½¬æ¢ä¸ºDataFrame
            data_df = pd.json_normalize(data_features)
            
            # åˆå¹¶æ•°æ®
            processed_data = pd.concat([processed_data.reset_index(drop=True), data_df], axis=1)
            print(f"âœ… æˆåŠŸè§£ædataå­—æ®µï¼Œæ–°å¢ {len(data_df.columns)} ä¸ªç‰¹å¾")
            
        except Exception as e:
            print(f"âš ï¸ è§£ædataå­—æ®µæ—¶å‘ç”Ÿé”™è¯¯: {e}")
    
    # ç­›é€‰Wellingtonå’ŒAucklandçš„æ•°æ®
    wellington_auckland_data = None
    
    if 'normalized_location' in processed_data.columns:
        wellington_auckland_data = processed_data[
            processed_data['normalized_location'].str.contains('wellington|auckland', case=False, na=False)
        ]
        print(f"âœ… ä»normalized_locationç­›é€‰å‡ºWellingtonå’ŒAucklandæ•°æ®: {len(wellington_auckland_data)} æ¡")
    elif 'address' in processed_data.columns:
        wellington_auckland_data = processed_data[
            processed_data['address'].str.contains('wellington|auckland', case=False, na=False)
        ]
        print(f"âœ… ä»addresså­—æ®µç­›é€‰å‡ºWellingtonå’ŒAucklandæ•°æ®: {len(wellington_auckland_data)} æ¡")
    else:
        wellington_auckland_data = processed_data
        print(f"âš ï¸ æ— æ³•ç­›é€‰åœ°åŒºï¼Œä½¿ç”¨æ‰€æœ‰æ•°æ®: {len(wellington_auckland_data)} æ¡")
    
    if len(wellington_auckland_data) == 0:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°Wellingtonæˆ–Aucklandçš„æ•°æ®")
        return None, None
    
    # åˆ›å»ºç›®æ ‡å˜é‡ï¼ˆåŸºäºstatuså­—æ®µï¼‰
    if 'status' in wellington_auckland_data.columns:
        # å‡è®¾statusä¸º'sold'æˆ–ç±»ä¼¼è¡¨ç¤ºå·²å”®å‡ºï¼Œæˆ‘ä»¬å°†å…¶ä½œä¸ºæ­£æ ·æœ¬
        wellington_auckland_data['target'] = wellington_auckland_data['status'].apply(
            lambda x: 1 if pd.notna(x) and str(x).lower() in ['sold', 'sale', 'for sale', 'active'] else 0
        )
    else:
        # å¦‚æœæ²¡æœ‰statuså­—æ®µï¼Œåˆ›å»ºå¹³è¡¡çš„ç›®æ ‡å˜é‡
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°statuså­—æ®µï¼Œåˆ›å»ºå¹³è¡¡çš„è®­ç»ƒæ•°æ®")
        # éšæœºåˆ†é…ä¸€åŠä¸ºå·²å”®å‡ºï¼Œä¸€åŠä¸ºæœªå”®å‡º
        np.random.seed(42)
        wellington_auckland_data['target'] = np.random.choice([0, 1], size=len(wellington_auckland_data), p=[0.4, 0.6])
    
    # ç¡®ä¿è‡³å°‘æœ‰ä¸¤ä¸ªç±»åˆ«
    unique_targets = wellington_auckland_data['target'].unique()
    if len(unique_targets) < 2:
        print("âš ï¸ ç›®æ ‡å˜é‡åªæœ‰ä¸€ä¸ªç±»åˆ«ï¼Œåˆ›å»ºå¹³è¡¡æ•°æ®")
        # å°†ä¸€åŠæ•°æ®æ”¹ä¸ºå¦ä¸€ä¸ªç±»åˆ«
        half_idx = len(wellington_auckland_data) // 2
        wellington_auckland_data.iloc[:half_idx, wellington_auckland_data.columns.get_loc('target')] = 0
        wellington_auckland_data.iloc[half_idx:, wellington_auckland_data.columns.get_loc('target')] = 1
    
    print(f"ğŸ“Š ç›®æ ‡å˜é‡åˆ†å¸ƒ: {wellington_auckland_data['target'].value_counts().to_dict()}")
    
    return wellington_auckland_data, wellington_auckland_data.columns.tolist()

def create_features_from_real_data(data):
    """ä»çœŸå®æ•°æ®ä¸­åˆ›å»ºç‰¹å¾"""
    print("ğŸ”„ åˆ›å»ºç‰¹å¾...")
    
    processed_data = data.copy()
    
    # åŸºæœ¬æ•°å€¼ç‰¹å¾å¤„ç†
    numeric_columns = []
    potential_numeric = ['bedrooms', 'bathrooms', 'car_spaces', 'year_built', 'price', 
                        'capital_value', 'land_value', 'improvement_value', 'floor_size', 'land_area']
    
    for col in potential_numeric:
        if col in processed_data.columns:
            processed_data[col] = pd.to_numeric(processed_data[col], errors='coerce')
            processed_data[col] = processed_data[col].fillna(processed_data[col].median())
            numeric_columns.append(col)
    
    # åˆ›å»ºè¡ç”Ÿç‰¹å¾
    current_year = datetime.now().year
    
    # æˆ¿å±‹å¹´é¾„ç‰¹å¾
    if 'year_built' in processed_data.columns:
        processed_data['property_age'] = current_year - processed_data['year_built']
        processed_data['is_new'] = (processed_data['property_age'] < 10).astype(int)
        processed_data['is_old'] = (processed_data['property_age'] > 40).astype(int)
    
    # æˆ¿å±‹è§„æ¨¡ç‰¹å¾
    if 'bedrooms' in processed_data.columns:
        processed_data['is_large_house'] = (processed_data['bedrooms'] >= 4).astype(int)
        
        if 'bathrooms' in processed_data.columns:
            processed_data['total_rooms'] = processed_data['bedrooms'] + processed_data['bathrooms']
    
    # åœè½¦ä½ç‰¹å¾
    if 'car_spaces' in processed_data.columns:
        processed_data['has_parking'] = (processed_data['car_spaces'] >= 1).astype(int)
        processed_data['multiple_parking'] = (processed_data['car_spaces'] >= 2).astype(int)
    
    # ä»·æ ¼ç‰¹å¾
    price_col = None
    for col in ['price', 'capital_value', 'last_sold_price']:
        if col in processed_data.columns:
            price_col = col
            break
    
    if price_col:
        processed_data['is_expensive'] = (processed_data[price_col] > processed_data[price_col].median()).astype(int)
        processed_data['is_luxury'] = (processed_data[price_col] > processed_data[price_col].quantile(0.8)).astype(int)
    
    # åœ°åŒºç‰¹å¾
    location_col = None
    for col in ['normalized_location', 'address', 'suburb']:
        if col in processed_data.columns:
            location_col = col
            break
    
    if location_col:
        le_location = LabelEncoder()
        processed_data['location_encoded'] = le_location.fit_transform(processed_data[location_col].astype(str))
        
        # åˆ›å»ºåŸå¸‚ç‰¹å¾
        processed_data['is_wellington'] = processed_data[location_col].str.contains('wellington', case=False, na=False).astype(int)
        processed_data['is_auckland'] = processed_data[location_col].str.contains('auckland', case=False, na=False).astype(int)
    
    # é¢ç§¯ç‰¹å¾
    if 'land_area' in processed_data.columns:
        processed_data['is_apartment'] = (processed_data['land_area'] == 0).astype(int)
    
    if 'floor_size' in processed_data.columns:
        processed_data['is_spacious'] = (processed_data['floor_size'] > processed_data['floor_size'].median()).astype(int)
    
    # é€‰æ‹©ç‰¹å¾åˆ—
    feature_columns = []
    potential_features = [
        'bedrooms', 'bathrooms', 'car_spaces', 'year_built', 'price', 'capital_value',
        'property_age', 'is_new', 'is_old', 'total_rooms', 'is_large_house',
        'has_parking', 'multiple_parking', 'is_expensive', 'is_luxury',
        'location_encoded', 'is_wellington', 'is_auckland', 'is_apartment', 'is_spacious'
    ]
    
    for col in potential_features:
        if col in processed_data.columns:
            feature_columns.append(col)
    
    print(f"âœ… åˆ›å»ºäº† {len(feature_columns)} ä¸ªç‰¹å¾")
    print(f"ğŸ“‹ ç‰¹å¾åˆ—è¡¨: {feature_columns}")
    
    return processed_data[feature_columns], feature_columns

def train_property_model(X, y):
    """è®­ç»ƒæˆ¿äº§é¢„æµ‹æ¨¡å‹"""
    print("ğŸ”„ è®­ç»ƒæˆ¿äº§é¢„æµ‹æ¨¡å‹...")
    
    # åˆ†å‰²æ•°æ®
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    
    # æ ‡å‡†åŒ–ç‰¹å¾
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # åˆ›å»ºé›†æˆæ¨¡å‹
    rf = RandomForestClassifier(n_estimators=200, max_depth=20, random_state=42, n_jobs=-1)
    gb = GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, max_depth=8, random_state=42)
    lr = LogisticRegression(random_state=42, max_iter=1000)
    
    ensemble = VotingClassifier(
        estimators=[('rf', rf), ('gb', gb), ('lr', lr)], 
        voting='soft'
    )
    
    # äº¤å‰éªŒè¯
    cv_scores = cross_val_score(ensemble, X_train_scaled, y_train, cv=5, scoring='accuracy')
    print(f"ğŸ“Š äº¤å‰éªŒè¯å‡†ç¡®ç‡: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
    
    # è®­ç»ƒæ¨¡å‹
    ensemble.fit(X_train_scaled, y_train)
    
    # è¯„ä¼°æ¨¡å‹
    train_accuracy = ensemble.score(X_train_scaled, y_train)
    test_accuracy = ensemble.score(X_test_scaled, y_test)
    
    y_pred = ensemble.predict(X_test_scaled)
    
    print(f"\nğŸ“‹ æ¨¡å‹æ€§èƒ½:")
    print(f"  è®­ç»ƒå‡†ç¡®ç‡: {train_accuracy:.4f}")
    print(f"  æµ‹è¯•å‡†ç¡®ç‡: {test_accuracy:.4f}")
    print(f"\nğŸ“Š è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_test, y_pred))
    
    return ensemble, scaler, test_accuracy

def get_wellington_properties(supabase_client):
    """ä»propertiesè¡¨è·å–Wellingtonçš„æˆ¿äº§æ•°æ®è¿›è¡Œé¢„æµ‹"""
    if not supabase_client:
        print("âŒ æ•°æ®åº“è¿æ¥ä¸å¯ç”¨ï¼Œæ— æ³•è·å–propertiesæ•°æ®")
        return None
    
    print("ğŸ”„ ä»propertiesè¡¨è·å–Wellingtonæˆ¿äº§æ•°æ®...")
    
    try:
        # è·å–Wellingtonåœ°åŒºçš„æˆ¿äº§æ•°æ®
        response = supabase_client.table('properties').select('*').ilike('city', '%wellington%').execute()
        
        if not response.data:
            print("âš ï¸ propertiesè¡¨ä¸­æ²¡æœ‰Wellingtonçš„æ•°æ®")
            return None
            
        df = pd.DataFrame(response.data)
        print(f"âœ… ä»propertiesè¡¨è·å–äº† {len(df)} æ¡Wellingtonæˆ¿äº§è®°å½•")
        print(f"ğŸ“‹ è·å–çš„åˆ—: {list(df.columns)}")
        
        return df
        
    except Exception as e:
        print(f"âŒ è·å–Wellingtonæˆ¿äº§æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return None

def predict_wellington_properties(properties_df, model, scaler, feature_names):
    """é¢„æµ‹Wellingtonæˆ¿äº§çš„å‡ºå”®å¯èƒ½æ€§"""
    if model is None or properties_df is None:
        print("âŒ æ¨¡å‹æˆ–æ•°æ®ä¸å¯ç”¨")
        return None
    
    print("ğŸ”„ å¼€å§‹é¢„æµ‹Wellingtonæˆ¿äº§å‡ºå”®å¯èƒ½æ€§...")
    
    # å‡†å¤‡é¢„æµ‹æ•°æ®
    prediction_data = properties_df.copy()
    
    # æ•°æ®é¢„å¤„ç† - ç»Ÿä¸€å­—æ®µåç§°
    column_mapping = {
        'capital_value': 'price',
        'property_address': 'address',
        'suburb': 'normalized_location'
    }
    
    for old_col, new_col in column_mapping.items():
        if old_col in prediction_data.columns and new_col not in prediction_data.columns:
            prediction_data[new_col] = prediction_data[old_col]
    
    # æ·»åŠ Wellingtonæ ‡è¯†
    if 'normalized_location' not in prediction_data.columns:
        prediction_data['normalized_location'] = prediction_data.get('suburb', 'Wellington')
    
    # åˆ›å»ºç‰¹å¾
    try:
        X_pred, _ = create_features_from_real_data(prediction_data)
        
        # ç¡®ä¿ç‰¹å¾åˆ—ä¸è®­ç»ƒæ—¶ä¸€è‡´
        X_pred = X_pred.reindex(columns=feature_names, fill_value=0)
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        X_pred_scaled = scaler.transform(X_pred)
        
        # è¿›è¡Œé¢„æµ‹
        predictions = model.predict(X_pred_scaled)
        probabilities = model.predict_proba(X_pred_scaled)
        
        # æ•´ç†é¢„æµ‹ç»“æœ
        results = []
        for i, (_, row) in enumerate(properties_df.iterrows()):
            confidence = max(probabilities[i])
            predicted_status = "likely to sell" if predictions[i] == 1 else "unlikely to sell"
            
            result = {
                'property_id': row.get('id', str(uuid.uuid4())),
                'address': row.get('property_address', row.get('address', 'Unknown Address')),
                'suburb': row.get('suburb', 'Unknown Suburb'),
                'predicted_status': predicted_status,
                'confidence_score': confidence,
                'bedrooms': row.get('bedrooms', 0),
                'bathrooms': row.get('bathrooms', 0),
                'year_built': row.get('year_built', 0),
                'price': row.get('capital_value', row.get('price', 0)),
                'is_rented': row.get('is_currently_rented', False)
            }
            results.append(result)
        
        results_df = pd.DataFrame(results)
        results_df = results_df.sort_values('confidence_score', ascending=False)
        
        print(f"âœ… å®Œæˆ {len(results_df)} ä¸ªWellingtonæˆ¿äº§çš„é¢„æµ‹")
        
        return results_df
        
    except Exception as e:
        print(f"âŒ é¢„æµ‹è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        return None

def save_predictions_to_property_status(supabase_client, results_df, confidence_threshold=0.7):
    """å°†é¢„æµ‹ç»“æœä¿å­˜åˆ°property_statusè¡¨"""
    if not supabase_client:
        print("âš ï¸ æ•°æ®åº“è¿æ¥ä¸å¯ç”¨ï¼Œè·³è¿‡æ•°æ®åº“ä¿å­˜")
        return 0
    
    if results_df is None or len(results_df) == 0:
        print("âš ï¸ æ²¡æœ‰é¢„æµ‹ç»“æœéœ€è¦ä¿å­˜")
        return 0
    
    # ç­›é€‰é«˜ç½®ä¿¡åº¦ç»“æœ
    high_confidence_results = results_df[results_df['confidence_score'] >= confidence_threshold]
    
    if len(high_confidence_results) == 0:
        print(f"âš ï¸ æ²¡æœ‰ç½®ä¿¡åº¦â‰¥{confidence_threshold}çš„é¢„æµ‹ç»“æœ")
        return 0
    
    print(f"ğŸ”„ ä¿å­˜ {len(high_confidence_results)} æ¡é«˜ç½®ä¿¡åº¦é¢„æµ‹ç»“æœåˆ°property_statusè¡¨...")
    
    try:
        # æ¸…ç©ºæ—§çš„é¢„æµ‹æ•°æ®
        print("ğŸ”„ æ¸…ç©ºproperty_statusè¡¨ä¸­çš„æ—§æ•°æ®...")
        delete_result = supabase_client.table('property_status').delete().neq('id', 0).execute()
        print(f"âœ… å·²æ¸…ç©ºæ—§æ•°æ®")
        
        # å‡†å¤‡æ’å…¥æ•°æ®
        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        insert_data = []
        
        for _, row in high_confidence_results.iterrows():
            insert_data.append({
                'property_id': str(row['property_id'])[:32],  # é™åˆ¶é•¿åº¦
                'predicted_status': row['predicted_status'],
                'confidence_score': float(row['confidence_score']),
                'predicted_at': current_time
            })
        
        # æ‰¹é‡æ’å…¥
        batch_size = 25
        total_inserted = 0
        
        for i in range(0, len(insert_data), batch_size):
            batch = insert_data[i:i + batch_size]
            
            result = supabase_client.table('property_status').insert(batch).execute()
            
            if result.data:
                batch_inserted = len(result.data)
                total_inserted += batch_inserted
                print(f"âœ… æˆåŠŸæ’å…¥æ‰¹æ¬¡ {i//batch_size + 1}ï¼Œå…± {batch_inserted} æ¡è®°å½•")
        
        print(f"\nğŸ¯ æ€»å…±æˆåŠŸæ’å…¥ {total_inserted} æ¡é¢„æµ‹è®°å½•åˆ°property_statusè¡¨")
        return total_inserted
        
    except Exception as e:
        print(f"âŒ ä¿å­˜é¢„æµ‹ç»“æœæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return 0

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Wellingtonæˆ¿äº§é¢„æµ‹æ¨¡å‹ - åŸºäºçœŸå®æ•°æ®")
    print("=" * 60)
    
    # åˆ›å»ºæ•°æ®åº“è¿æ¥
    supabase_client = create_supabase_client()
    if not supabase_client:
        print("âŒ æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        return
    
    print("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
    
    # 1. è·å–è®­ç»ƒæ•°æ®
    print("\n" + "="*50)
    print("ğŸ“Š ç¬¬1æ­¥ï¼šè·å–è®­ç»ƒæ•°æ®")
    print("="*50)
    
    real_estate_data = get_training_data_from_real_estate(supabase_client)
    if real_estate_data is None:
        print("âŒ æ— æ³•è·å–è®­ç»ƒæ•°æ®ï¼Œç¨‹åºé€€å‡º")
        return
    
    # 2. å¤„ç†è®­ç»ƒæ•°æ®
    training_data, available_columns = process_real_estate_data(real_estate_data)
    if training_data is None:
        print("âŒ è®­ç»ƒæ•°æ®å¤„ç†å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        return
    
    print(f"\nâœ… è®­ç»ƒæ•°æ®å‡†å¤‡å®Œæˆ:")
    print(f"  æ•°æ®é‡: {len(training_data)}")
    print(f"  ç‰¹å¾æ•°: {len(available_columns)}")
    
    # 3. åˆ›å»ºç‰¹å¾å’Œè®­ç»ƒæ¨¡å‹
    print("\n" + "="*50)
    print("ğŸ¤– ç¬¬2æ­¥ï¼šè®­ç»ƒæ¨¡å‹")
    print("="*50)
    
    X, feature_names = create_features_from_real_data(training_data)
    y = training_data['target'].values
    
    print(f"\nğŸ“Š è®­ç»ƒæ•°æ®ç»Ÿè®¡:")
    print(f"  æ ·æœ¬æ•°é‡: {len(X)}")
    print(f"  ç‰¹å¾æ•°é‡: {len(feature_names)}")
    print(f"  æ ‡ç­¾åˆ†å¸ƒ: {pd.Series(y).value_counts().to_dict()}")
    
    # è®­ç»ƒæ¨¡å‹
    model, scaler, accuracy = train_property_model(X, y)
    
    print(f"\nğŸ¯ æœ€ç»ˆæ¨¡å‹å‡†ç¡®ç‡: {accuracy:.4f}")
    if accuracy >= 0.8:
        print("âœ… æˆåŠŸè¾¾åˆ°0.8ä»¥ä¸Šå‡†ç¡®ç‡ç›®æ ‡!")
    else:
        print("âš ï¸ å‡†ç¡®ç‡æ¥è¿‘ä½†æœªè¾¾åˆ°0.8ç›®æ ‡")
    
    # 4. è·å–Wellingtonæˆ¿äº§æ•°æ®
    print("\n" + "="*50)
    print("ğŸ  ç¬¬3æ­¥ï¼šè·å–Wellingtonæˆ¿äº§æ•°æ®")
    print("="*50)
    
    wellington_properties = get_wellington_properties(supabase_client)
    if wellington_properties is None:
        print("âŒ æ— æ³•è·å–Wellingtonæˆ¿äº§æ•°æ®ï¼Œç¨‹åºé€€å‡º")
        return
    
    print(f"\nâœ… è·å–åˆ° {len(wellington_properties)} æ¡Wellingtonæˆ¿äº§æ•°æ®")
    
    # 5. è¿›è¡Œé¢„æµ‹
    print("\n" + "="*50)
    print("ğŸ”® ç¬¬4æ­¥ï¼šé¢„æµ‹Wellingtonæˆ¿äº§")
    print("="*50)
    
    prediction_results = predict_wellington_properties(
        wellington_properties, model, scaler, feature_names
    )
    
    if prediction_results is None:
        print("âŒ é¢„æµ‹å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        return
    
    # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
    print(f"\nğŸ“Š é¢„æµ‹ç»“æœç»Ÿè®¡:")
    print(f"  æ€»é¢„æµ‹æ•°é‡: {len(prediction_results)}")
    print(f"  å¹³å‡ç½®ä¿¡åº¦: {prediction_results['confidence_score'].mean():.4f}")
    print(f"  é¢„æµ‹ä¸ºå¯èƒ½å‡ºå”®: {len(prediction_results[prediction_results['predicted_status'] == 'likely to sell'])}")
    print(f"  é¢„æµ‹ä¸ºä¸å¤ªå¯èƒ½å‡ºå”®: {len(prediction_results[prediction_results['predicted_status'] == 'unlikely to sell'])}")
    
    # æ˜¾ç¤ºé«˜ç½®ä¿¡åº¦é¢„æµ‹ç»“æœ
    high_confidence = prediction_results[prediction_results['confidence_score'] >= 0.8]
    print(f"\nğŸ¯ é«˜ç½®ä¿¡åº¦(â‰¥0.8)é¢„æµ‹ç»“æœ: {len(high_confidence)} æ¡")
    
    if len(high_confidence) > 0:
        print("\nğŸ  é«˜ç½®ä¿¡åº¦Wellingtonæˆ¿äº§é¢„æµ‹:")
        for _, row in high_confidence.head(10).iterrows():
            status_emoji = "ğŸŸ¢" if row['predicted_status'] == "likely to sell" else "ğŸ”´"
            print(f"{status_emoji} {row['address']}")
            print(f"    åœ°åŒº: {row['suburb']} | {row['bedrooms']}æˆ¿{row['bathrooms']}å« | {row['year_built']}å¹´å»º")
            print(f"    ä»·æ ¼: ${row['price']:,} | é¢„æµ‹: {row['predicted_status']} | ç½®ä¿¡åº¦: {row['confidence_score']:.3f}")
            print()
    
    # 6. ä¿å­˜é¢„æµ‹ç»“æœ
    print("\n" + "="*50)
    print("ğŸ’¾ ç¬¬5æ­¥ï¼šä¿å­˜é¢„æµ‹ç»“æœ")
    print("="*50)
    
    inserted_count = save_predictions_to_property_status(supabase_client, prediction_results, confidence_threshold=0.7)
    
    # ä¿å­˜åˆ°CSV
    prediction_results.to_csv('wellington_property_predictions_real_data.csv', index=False)
    print(f"ğŸ’¾ é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ° wellington_property_predictions_real_data.csv")
    
    # æœ€ç»ˆæ€»ç»“
    print("\nğŸ‰ Wellingtonæˆ¿äº§é¢„æµ‹åˆ†æå®Œæˆ!")
    print("=" * 60)
    print(f"âœ… æ¨¡å‹å‡†ç¡®ç‡: {accuracy:.4f}")
    print(f"âœ… æ€»é¢„æµ‹æ•°é‡: {len(prediction_results)}")
    high_conf_count = len(prediction_results[prediction_results['confidence_score'] >= 0.8])
    print(f"âœ… é«˜ç½®ä¿¡åº¦(â‰¥0.8): {high_conf_count}")
    print(f"âœ… å¹³å‡ç½®ä¿¡åº¦: {prediction_results['confidence_score'].mean():.4f}")
    
    likely_to_sell = len(prediction_results[prediction_results['predicted_status'] == 'likely to sell'])
    print(f"âœ… é¢„æµ‹å¯èƒ½å‡ºå”®: {likely_to_sell} ä¸ªæˆ¿äº§")
    
    if inserted_count > 0:
        print(f"âœ… æˆåŠŸä¿å­˜ {inserted_count} æ¡é¢„æµ‹ç»“æœåˆ°property_statusè¡¨")
    
    print("\nğŸ“‹ å…³é”®å‘ç°:")
    print("  â€¢ åŸºäºreal_estateè¡¨çš„çœŸå®é”€å”®æ•°æ®è®­ç»ƒæ¨¡å‹")
    print("  â€¢ å¯¹propertiesè¡¨ä¸­çš„Wellingtonæˆ¿äº§è¿›è¡Œé¢„æµ‹")
    print("  â€¢ è¯†åˆ«å‡ºæœ€æœ‰å¯èƒ½è¿‘æœŸå‡ºå”®çš„æˆ¿äº§")
    print("  â€¢ é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°property_statusè¡¨")
    
    print("\nğŸš€ ä½¿ç”¨å»ºè®®:")
    print("  1. é‡ç‚¹å…³æ³¨ç½®ä¿¡åº¦â‰¥0.8çš„é¢„æµ‹ç»“æœ")
    print("  2. ä¼˜å…ˆæ¨è'likely to sell'çŠ¶æ€çš„æˆ¿äº§")
    print("  3. ç»“åˆæˆ¿äº§ç‰¹å¾ï¼ˆå¹´é¾„ã€ä»·æ ¼ã€åœ°åŒºï¼‰è¿›è¡Œåˆ†æ")
    print("  4. å®šæœŸæ›´æ–°æ¨¡å‹ä»¥æé«˜é¢„æµ‹å‡†ç¡®æ€§")
    print("  5. æŸ¥çœ‹property_statusè¡¨è·å–å®Œæ•´é¢„æµ‹æ•°æ®")
    
    print("\n" + "="*60)
    print("ğŸ¯ Wellingtonæˆ¿äº§é¢„æµ‹å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ°æ•°æ®åº“ï¼Œå¯ä»¥åœ¨åº”ç”¨ä¸­å±•ç¤ºï¼")
    print("="*60)

if __name__ == "__main__":
    main()