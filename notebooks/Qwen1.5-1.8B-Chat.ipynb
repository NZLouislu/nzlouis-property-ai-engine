{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate gradio torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model page: https://huggingface.co/Qwen/Qwen1.5-1.8B-Chat\n",
    "\n",
    "âš ï¸ If the generated code snippets do not work, please open an issue on either the model repo and/or on huggingface.js ğŸ™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.float16 if device == \"cuda\" else torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# è‡ªåŠ¨é€‰æ‹©è®¾å¤‡å’Œç²¾åº¦\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
    "\n",
    "# æ¨¡å‹åç§°\n",
    "model_name = \"Qwen/Qwen1.5-1.8B-Chat\"\n",
    "hf_token = os.getenv(\"HF_TOKEN\")  # å¯é€‰ï¼šå¦‚æœä½ æœ‰ç§æœ‰æ¨¡å‹è®¿é—®æƒé™\n",
    "\n",
    "# åŠ è½½ tokenizer å’Œæ¨¡å‹\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=hf_token, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\" if device == \"cuda\" else None,\n",
    "    torch_dtype=dtype,\n",
    "    use_auth_token=hf_token,\n",
    "    trust_remote_code=True\n",
    ").to(device)\n",
    "\n",
    "# èŠå¤©å‡½æ•°ï¼ˆæ”¯æŒå¤šè½®ï¼‰\n",
    "def chat(message, history):\n",
    "    try:\n",
    "        # å°† history ä»å…ƒç»„è½¬æ¢ä¸º OpenAI é£æ ¼æ ¼å¼\n",
    "        messages = []\n",
    "        for user_msg, bot_msg in history:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": bot_msg})\n",
    "\n",
    "        # æ·»åŠ å½“å‰ç”¨æˆ·è¾“å…¥\n",
    "        messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "        # æ„å»ºè¾“å…¥æ–‡æœ¬\n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # ç”Ÿæˆå›å¤\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=512,\n",
    "            do_sample=True,\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "        # æ›´æ–°å†å²ï¼ˆä»ä½¿ç”¨å…ƒç»„æ ¼å¼ä¾› Gradio æ˜¾ç¤ºï¼‰\n",
    "        history.append((message, response))\n",
    "        return \"\", history, history\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ é”™è¯¯ä¿¡æ¯ï¼š{e}\")\n",
    "        error_msg = f\"å‘ç”Ÿé”™è¯¯ï¼š{str(e)}\"\n",
    "        history.append((message, error_msg))\n",
    "        return \"\", history, history\n",
    "\n",
    "# Gradio ç•Œé¢\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## ğŸ¤– Qwen1.5-1.8B ä¸­æ–‡ Chatbot\")\n",
    "    chatbot = gr.Chatbot(label=\"å¯¹è¯è®°å½•\", type=\"tuples\")\n",
    "    msg = gr.Textbox(label=\"è¯·è¾“å…¥ä½ çš„é—®é¢˜\", placeholder=\"ä¾‹å¦‚ï¼šä½ å¥½ï¼Œä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ\", lines=2)\n",
    "    submit_btn = gr.Button(\"æäº¤\")\n",
    "    state = gr.State([])\n",
    "\n",
    "    submit_btn.click(chat, [msg, state], [msg, chatbot, state])\n",
    "    msg.submit(chat, [msg, state], [msg, chatbot, state])\n",
    "\n",
    "demo.launch(debug=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
