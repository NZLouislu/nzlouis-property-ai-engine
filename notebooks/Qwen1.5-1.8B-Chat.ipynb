{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate gradio torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model page: https://huggingface.co/Qwen/Qwen1.5-1.8B-Chat\n",
    "\n",
    "⚠️ If the generated code snippets do not work, please open an issue on either the model repo and/or on huggingface.js 🙏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.float16 if device == \"cuda\" else torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# 自动选择设备和精度\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
    "\n",
    "# 模型名称\n",
    "model_name = \"Qwen/Qwen1.5-1.8B-Chat\"\n",
    "hf_token = os.getenv(\"HF_TOKEN\")  # 可选：如果你有私有模型访问权限\n",
    "\n",
    "# 加载 tokenizer 和模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=hf_token, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\" if device == \"cuda\" else None,\n",
    "    torch_dtype=dtype,\n",
    "    use_auth_token=hf_token,\n",
    "    trust_remote_code=True\n",
    ").to(device)\n",
    "\n",
    "# 聊天函数（支持多轮）\n",
    "def chat(message, history):\n",
    "    try:\n",
    "        # 将 history 从元组转换为 OpenAI 风格格式\n",
    "        messages = []\n",
    "        for user_msg, bot_msg in history:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": bot_msg})\n",
    "\n",
    "        # 添加当前用户输入\n",
    "        messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "        # 构建输入文本\n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # 生成回复\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=512,\n",
    "            do_sample=True,\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "        # 更新历史（仍使用元组格式供 Gradio 显示）\n",
    "        history.append((message, response))\n",
    "        return \"\", history, history\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 错误信息：{e}\")\n",
    "        error_msg = f\"发生错误：{str(e)}\"\n",
    "        history.append((message, error_msg))\n",
    "        return \"\", history, history\n",
    "\n",
    "# Gradio 界面\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## 🤖 Qwen1.5-1.8B 中文 Chatbot\")\n",
    "    chatbot = gr.Chatbot(label=\"对话记录\", type=\"tuples\")\n",
    "    msg = gr.Textbox(label=\"请输入你的问题\", placeholder=\"例如：你好，今天天气怎么样？\", lines=2)\n",
    "    submit_btn = gr.Button(\"提交\")\n",
    "    state = gr.State([])\n",
    "\n",
    "    submit_btn.click(chat, [msg, state], [msg, chatbot, state])\n",
    "    msg.submit(chat, [msg, state], [msg, chatbot, state])\n",
    "\n",
    "demo.launch(debug=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
