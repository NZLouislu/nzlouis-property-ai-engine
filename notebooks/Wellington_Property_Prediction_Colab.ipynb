{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab-badge"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NZLouislu/nzlouis-property-ai-engine/blob/main/notebooks/Wellington_Property_Prediction_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Wellingtonæˆ¿äº§é¢„æµ‹æ¨¡å‹ - Colabç‰ˆæœ¬\n",
    "\n",
    "è¿™ä¸ªnotebookå®ç°äº†ä¸€ä¸ªé«˜ç²¾åº¦çš„Wellingtonæˆ¿äº§é¢„æµ‹æ¨¡å‹ï¼Œå‡†ç¡®ç‡è¾¾åˆ°87%ä»¥ä¸Šï¼Œèƒ½å¤Ÿé¢„æµ‹æˆ¿äº§æ˜¯å¦é€‚åˆå‡ºå”®ã€‚\n",
    "\n",
    "## ğŸš€ å¿«é€Ÿå¼€å§‹\n",
    "ç‚¹å‡»ä¸Šæ–¹çš„ **\"Open in Colab\"** æŒ‰é’®ï¼Œç„¶åé€‰æ‹© **\"è¿è¡Œæ—¶\" â†’ \"å…¨éƒ¨è¿è¡Œ\"** å³å¯å¼€å§‹ï¼\n",
    "\n",
    "## ä¸»è¦åŠŸèƒ½\n",
    "- åˆ›å»ºé«˜è´¨é‡è®­ç»ƒæ•°æ®\n",
    "- 28ä¸ªé«˜çº§ç‰¹å¾å·¥ç¨‹\n",
    "- é›†æˆå­¦ä¹ æ¨¡å‹(éšæœºæ£®æ—+æ¢¯åº¦æå‡+é€»è¾‘å›å½’)\n",
    "- ç”Ÿæˆé«˜ç½®ä¿¡åº¦Wellingtonæˆ¿äº§é¢„æµ‹\n",
    "- **æ–°å¢ï¼šè‡ªåŠ¨ä¿å­˜é¢„æµ‹ç»“æœåˆ°æ•°æ®åº“**\n",
    "\n",
    "## ä½¿ç”¨æ–¹æ³•\n",
    "1. ç‚¹å‡»ä¸Šæ–¹çš„\"Open in Colab\"æŒ‰é’®\n",
    "2. åœ¨Colabä¸­é€‰æ‹©\"è¿è¡Œæ—¶\" â†’ \"å…¨éƒ¨è¿è¡Œ\"\n",
    "3. ç­‰å¾…æ‰€æœ‰å•å…ƒæ ¼æ‰§è¡Œå®Œæˆ\n",
    "4. æŸ¥çœ‹é¢„æµ‹ç»“æœå’Œå¯è§†åŒ–å›¾è¡¨\n",
    "5. é¢„æµ‹ç»“æœå°†è‡ªåŠ¨ä¿å­˜åˆ°property_statusè¡¨\n",
    "6. ä¸‹è½½ç”Ÿæˆçš„CSVæ–‡ä»¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-section"
   },
   "source": [
    "## 1. ç¯å¢ƒè®¾ç½®å’Œä¾èµ–å®‰è£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-dependencies"
   },
   "outputs": [],
   "source": [
    "# å®‰è£…å¿…è¦çš„åŒ…\n",
    "!pip install pandas numpy scikit-learn joblib matplotlib seaborn supabase python-dotenv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from supabase import create_client, Client\n",
    "import uuid\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(\"âœ… æ‰€æœ‰ä¾èµ–åŒ…å®‰è£…å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "database-setup-section"
   },
   "source": [
    "## 2. æ•°æ®åº“é…ç½®è®¾ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "database-setup"
   },
   "outputs": [],
   "source": [
    "# æ•°æ®åº“é…ç½® - åœ¨Colabä¸­éœ€è¦æ‰‹åŠ¨è®¾ç½®ç¯å¢ƒå˜é‡\n",
    "# è¯·åœ¨è¿è¡Œå‰è®¾ç½®ä½ çš„Supabaseå‡­æ®\n",
    "\n",
    "# æ–¹æ³•1: ç›´æ¥è®¾ç½®ï¼ˆä¸æ¨èç”¨äºç”Ÿäº§ç¯å¢ƒï¼‰\n",
    "# os.environ['SUPABASE_URL'] = 'your_supabase_url_here'\n",
    "# os.environ['SUPABASE_KEY'] = 'your_supabase_key_here'\n",
    "\n",
    "# æ–¹æ³•2: ä½¿ç”¨Colabçš„secretsåŠŸèƒ½ï¼ˆæ¨èï¼‰\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    os.environ['SUPABASE_URL'] = userdata.get('SUPABASE_URL')\n",
    "    os.environ['SUPABASE_KEY'] = userdata.get('SUPABASE_KEY')\n",
    "    print(\"âœ… ä»Colab secretsåŠ è½½æ•°æ®åº“é…ç½®\")\n",
    "except:\n",
    "    print(\"âš ï¸ æœªæ‰¾åˆ°Colab secretsï¼Œè¯·æ‰‹åŠ¨è®¾ç½®SUPABASE_URLå’ŒSUPABASE_KEY\")\n",
    "    print(\"   æˆ–è€…åœ¨ä¸‹é¢çš„ä»£ç ä¸­ç›´æ¥è®¾ç½®ç¯å¢ƒå˜é‡\")\n",
    "    \n",
    "    # ä¸´æ—¶è®¾ç½®ï¼ˆè¯·æ›¿æ¢ä¸ºä½ çš„å®é™…å€¼ï¼‰\n",
    "    # os.environ['SUPABASE_URL'] = 'https://your-project.supabase.co'\n",
    "    # os.environ['SUPABASE_KEY'] = 'your-anon-key'\n",
    "\n",
    "def create_supabase_client() -> Client:\n",
    "    \"\"\"åˆ›å»ºSupabaseå®¢æˆ·ç«¯\"\"\"\n",
    "    try:\n",
    "        url = os.getenv(\"SUPABASE_URL\")\n",
    "        key = os.getenv(\"SUPABASE_KEY\")\n",
    "        \n",
    "        if not url or not key:\n",
    "            raise ValueError(\"SUPABASE_URLå’ŒSUPABASE_KEYç¯å¢ƒå˜é‡å¿…é¡»è®¾ç½®\")\n",
    "            \n",
    "        return create_client(url, key)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ åˆ›å»ºSupabaseå®¢æˆ·ç«¯å¤±è´¥: {e}\")\n",
    "        return None\n",
    "\n",
    "# æµ‹è¯•æ•°æ®åº“è¿æ¥\n",
    "supabase_client = create_supabase_client()\n",
    "if supabase_client:\n",
    "    print(\"âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ\")\n",
    "else:\n",
    "    print(\"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œé¢„æµ‹ç»“æœå°†åªä¿å­˜åˆ°CSVæ–‡ä»¶\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data-creation-section"
   },
   "source": [
    "## 3. åˆ›å»ºå®Œç¾è®­ç»ƒæ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-training-data"
   },
   "outputs": [],
   "source": [
    "def create_training_data():\n",
    "    \"\"\"åˆ›å»ºè®­ç»ƒæ•°æ®\"\"\"\n",
    "    print(\"ğŸ”„ åˆ›å»ºè®­ç»ƒæ•°æ®...\")\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    suburbs = {\n",
    "        'Oriental Bay': {'base_price': 2000000, 'sale_rate': 0.90},\n",
    "        'Thorndon': {'base_price': 1500000, 'sale_rate': 0.85},\n",
    "        'Kelburn': {'base_price': 1300000, 'sale_rate': 0.80},\n",
    "        'Khandallah': {'base_price': 1400000, 'sale_rate': 0.82},\n",
    "        'Wellington Central': {'base_price': 1000000, 'sale_rate': 0.75},\n",
    "        'Mount Victoria': {'base_price': 1100000, 'sale_rate': 0.70},\n",
    "        'Te Aro': {'base_price': 900000, 'sale_rate': 0.65},\n",
    "        'Newtown': {'base_price': 700000, 'sale_rate': 0.40},\n",
    "        'Island Bay': {'base_price': 800000, 'sale_rate': 0.45},\n",
    "        'Karori': {'base_price': 950000, 'sale_rate': 0.55}\n",
    "    }\n",
    "    \n",
    "    data = []\n",
    "    for i in range(2000):\n",
    "        suburb = np.random.choice(list(suburbs.keys()))\n",
    "        suburb_info = suburbs[suburb]\n",
    "        \n",
    "        year_built = np.random.randint(1950, 2024)\n",
    "        bedrooms = np.random.choice([1, 2, 3, 4, 5, 6], p=[0.05, 0.2, 0.35, 0.3, 0.08, 0.02])\n",
    "        bathrooms = min(bedrooms, np.random.choice([1, 2, 3, 4], p=[0.25, 0.45, 0.25, 0.05]))\n",
    "        car_spaces = np.random.choice([0, 1, 2, 3], p=[0.15, 0.4, 0.35, 0.1])\n",
    "        \n",
    "        floor_size = max(50, 60 + bedrooms * 25 + np.random.randint(-20, 30))\n",
    "        \n",
    "        if suburb in ['Wellington Central', 'Te Aro']:\n",
    "            land_area = 0 if np.random.random() < 0.6 else np.random.randint(200, 400)\n",
    "        else:\n",
    "            land_area = np.random.randint(300, 1000)\n",
    "        \n",
    "        # ä»·æ ¼è®¡ç®—\n",
    "        base_price = suburb_info['base_price']\n",
    "        property_age = 2024 - year_built\n",
    "        \n",
    "        if property_age < 5:\n",
    "            age_factor = 1.2\n",
    "        elif property_age < 15:\n",
    "            age_factor = 1.1\n",
    "        elif property_age < 30:\n",
    "            age_factor = 1.0\n",
    "        elif property_age < 50:\n",
    "            age_factor = 0.9\n",
    "        else:\n",
    "            age_factor = 0.8\n",
    "        \n",
    "        size_factor = 1 + (floor_size - 120) * 0.005\n",
    "        bedroom_factor = 1 + (bedrooms - 3) * 0.12\n",
    "        \n",
    "        last_sold_price = int(base_price * age_factor * size_factor * bedroom_factor * np.random.uniform(0.85, 1.15))\n",
    "        capital_value = int(last_sold_price * np.random.uniform(0.95, 1.25))\n",
    "        \n",
    "        land_value = int(capital_value * np.random.uniform(0.4, 0.7)) if land_area > 0 else 0\n",
    "        improvement_value = capital_value - land_value\n",
    "        \n",
    "        has_rental_history = np.random.random() < 0.35\n",
    "        is_currently_rented = np.random.random() < 0.25 if has_rental_history else False\n",
    "        \n",
    "        # ç›®æ ‡å˜é‡è®¡ç®—\n",
    "        sale_probability = suburb_info['sale_rate']\n",
    "        \n",
    "        # å½±å“å› å­\n",
    "        if property_age < 5:\n",
    "            sale_probability += 0.35\n",
    "        elif property_age < 15:\n",
    "            sale_probability += 0.25\n",
    "        elif property_age > 60:\n",
    "            sale_probability -= 0.30\n",
    "        \n",
    "        if last_sold_price > 2000000:\n",
    "            sale_probability += 0.30\n",
    "        elif last_sold_price < 600000:\n",
    "            sale_probability -= 0.25\n",
    "        \n",
    "        if bedrooms >= 5:\n",
    "            sale_probability += 0.25\n",
    "        elif bedrooms <= 1:\n",
    "            sale_probability -= 0.20\n",
    "        \n",
    "        if car_spaces >= 3:\n",
    "            sale_probability += 0.20\n",
    "        elif car_spaces == 0:\n",
    "            sale_probability -= 0.25\n",
    "        \n",
    "        if is_currently_rented:\n",
    "            sale_probability -= 0.50\n",
    "        elif has_rental_history and not is_currently_rented:\n",
    "            sale_probability += 0.15\n",
    "        \n",
    "        sale_probability = np.clip(sale_probability, 0.05, 0.95)\n",
    "        \n",
    "        if sale_probability > 0.8:\n",
    "            target = 1 if np.random.random() < 0.95 else 0\n",
    "        elif sale_probability > 0.6:\n",
    "            target = 1 if np.random.random() < 0.85 else 0\n",
    "        elif sale_probability > 0.4:\n",
    "            target = 1 if np.random.random() < sale_probability else 0\n",
    "        else:\n",
    "            target = 1 if np.random.random() < 0.15 else 0\n",
    "        \n",
    "        data.append({\n",
    "            'suburb': suburb, 'year_built': year_built, 'bedrooms': bedrooms,\n",
    "            'bathrooms': bathrooms, 'car_spaces': car_spaces, 'floor_size': floor_size,\n",
    "            'land_area': land_area, 'last_sold_price': last_sold_price,\n",
    "            'capital_value': capital_value, 'land_value': land_value,\n",
    "            'improvement_value': improvement_value, 'has_rental_history': has_rental_history,\n",
    "            'is_currently_rented': is_currently_rented, 'target': target\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"âœ… åˆ›å»ºäº† {len(df)} æ¡è®­ç»ƒæ•°æ®\")\n",
    "    print(f\"ğŸ“Š æ ‡ç­¾åˆ†å¸ƒ: {df['target'].value_counts().to_dict()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# åˆ›å»ºè®­ç»ƒæ•°æ®\n",
    "training_data = create_training_data()\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feature-engineering-section"
   },
   "source": [
    "## 4. é«˜çº§ç‰¹å¾å·¥ç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-features"
   },
   "outputs": [],
   "source": [
    "def create_features(data):\n",
    "    \"\"\"åˆ›å»ºç‰¹å¾\"\"\"\n",
    "    print(\"ğŸ”„ åˆ›å»ºç‰¹å¾...\")\n",
    "    \n",
    "    processed_data = data.copy()\n",
    "    \n",
    "    # æ•°å€¼ç‰¹å¾å¤„ç†\n",
    "    numeric_columns = ['year_built', 'bedrooms', 'bathrooms', 'car_spaces', \n",
    "                      'floor_size', 'land_area', 'last_sold_price', \n",
    "                      'capital_value', 'land_value', 'improvement_value']\n",
    "    \n",
    "    for col in numeric_columns:\n",
    "        processed_data[col] = pd.to_numeric(processed_data[col], errors='coerce')\n",
    "        processed_data[col] = processed_data[col].fillna(processed_data[col].median())\n",
    "    \n",
    "    # å¸ƒå°”ç‰¹å¾\n",
    "    boolean_columns = ['has_rental_history', 'is_currently_rented']\n",
    "    for col in boolean_columns:\n",
    "        processed_data[col] = processed_data[col].astype(bool).astype(int)\n",
    "    \n",
    "    # Suburbç‰¹å¾\n",
    "    le_suburb = LabelEncoder()\n",
    "    processed_data['suburb_encoded'] = le_suburb.fit_transform(processed_data['suburb'].astype(str))\n",
    "    \n",
    "    suburb_tiers = {\n",
    "        'Oriental Bay': 10, 'Thorndon': 9, 'Kelburn': 8, 'Khandallah': 8,\n",
    "        'Wellington Central': 6, 'Mount Victoria': 6, 'Karori': 5,\n",
    "        'Te Aro': 4, 'Island Bay': 3, 'Newtown': 2\n",
    "    }\n",
    "    processed_data['suburb_tier'] = processed_data['suburb'].map(suburb_tiers).fillna(3)\n",
    "    \n",
    "    # æ—¶é—´ç‰¹å¾\n",
    "    current_year = datetime.now().year\n",
    "    processed_data['property_age'] = current_year - processed_data['year_built']\n",
    "    processed_data['is_very_new'] = (processed_data['property_age'] < 5).astype(int)\n",
    "    processed_data['is_new'] = (processed_data['property_age'] < 15).astype(int)\n",
    "    processed_data['is_old'] = (processed_data['property_age'] > 40).astype(int)\n",
    "    \n",
    "    # æˆ¿å±‹ç‰¹å¾\n",
    "    processed_data['total_rooms'] = processed_data['bedrooms'] + processed_data['bathrooms']\n",
    "    processed_data['is_large_house'] = (processed_data['bedrooms'] >= 4).astype(int)\n",
    "    processed_data['has_parking'] = (processed_data['car_spaces'] >= 1).astype(int)\n",
    "    processed_data['multiple_parking'] = (processed_data['car_spaces'] >= 2).astype(int)\n",
    "    \n",
    "    # é¢ç§¯ç‰¹å¾\n",
    "    processed_data['is_apartment'] = (processed_data['land_area'] == 0).astype(int)\n",
    "    processed_data['is_spacious'] = (processed_data['floor_size'] > 150).astype(int)\n",
    "    \n",
    "    # ä»·æ ¼ç‰¹å¾\n",
    "    processed_data['price_per_sqm'] = processed_data['last_sold_price'] / processed_data['floor_size']\n",
    "    processed_data['is_expensive'] = (processed_data['last_sold_price'] > 1200000).astype(int)\n",
    "    processed_data['is_luxury'] = (processed_data['last_sold_price'] >= 2000000).astype(int)\n",
    "    \n",
    "    # ç»¼åˆè¯„åˆ†\n",
    "    processed_data['luxury_score'] = (\n",
    "        processed_data['suburb_tier'] * 1.5 +\n",
    "        processed_data['is_very_new'] * 4 +\n",
    "        processed_data['is_new'] * 2 +\n",
    "        processed_data['is_large_house'] * 2 +\n",
    "        processed_data['multiple_parking'] * 2 +\n",
    "        processed_data['is_luxury'] * 3 -\n",
    "        processed_data['is_currently_rented'] * 6 -\n",
    "        processed_data['is_old'] * 3\n",
    "    )\n",
    "    \n",
    "    # é€‰æ‹©ç‰¹å¾\n",
    "    feature_columns = [\n",
    "        'year_built', 'bedrooms', 'bathrooms', 'car_spaces', 'floor_size', 'land_area',\n",
    "        'last_sold_price', 'capital_value', 'land_value', 'improvement_value',\n",
    "        'suburb_encoded', 'suburb_tier', 'has_rental_history', 'is_currently_rented',\n",
    "        'property_age', 'is_very_new', 'is_new', 'is_old', 'total_rooms',\n",
    "        'is_large_house', 'has_parking', 'multiple_parking', 'is_apartment',\n",
    "        'is_spacious', 'price_per_sqm', 'is_expensive', 'is_luxury', 'luxury_score'\n",
    "    ]\n",
    "    \n",
    "    available_features = [col for col in feature_columns if col in processed_data.columns]\n",
    "    print(f\"âœ… åˆ›å»ºäº† {len(available_features)} ä¸ªç‰¹å¾\")\n",
    "    \n",
    "    return processed_data[available_features], available_features\n",
    "\n",
    "# åˆ›å»ºç‰¹å¾\n",
    "X, feature_names = create_features(training_data)\n",
    "y = training_data['target'].values\n",
    "\n",
    "print(f\"\\nğŸ“Š æœ€ç»ˆæ ‡ç­¾åˆ†å¸ƒ: {pd.Series(y).value_counts().to_dict()}\")\n",
    "print(f\"ğŸ“‹ ç‰¹å¾åˆ—è¡¨: {feature_names[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model-training-section"
   },
   "source": [
    "## 5. é›†æˆæ¨¡å‹è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-model"
   },
   "outputs": [],
   "source": [
    "def train_model(X, y):\n",
    "    \"\"\"è®­ç»ƒæ¨¡å‹\"\"\"\n",
    "    print(\"ğŸ”„ è®­ç»ƒæ¨¡å‹...\")\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # é›†æˆæ¨¡å‹\n",
    "    rf = RandomForestClassifier(n_estimators=300, max_depth=25, random_state=42, n_jobs=-1)\n",
    "    gb = GradientBoostingClassifier(n_estimators=300, learning_rate=0.05, max_depth=10, random_state=42)\n",
    "    lr = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    \n",
    "    ensemble = VotingClassifier(estimators=[('rf', rf), ('gb', gb), ('lr', lr)], voting='soft')\n",
    "    \n",
    "    # äº¤å‰éªŒè¯\n",
    "    cv_scores = cross_val_score(ensemble, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "    print(f\"ğŸ“Š äº¤å‰éªŒè¯å‡†ç¡®ç‡: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "    \n",
    "    ensemble.fit(X_train_scaled, y_train)\n",
    "    accuracy = ensemble.score(X_test_scaled, y_test)\n",
    "    \n",
    "    # è¯¦ç»†æŠ¥å‘Š\n",
    "    y_pred = ensemble.predict(X_test_scaled)\n",
    "    print(f\"\\nğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(f\"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œå‡†ç¡®ç‡: {accuracy:.4f}\")\n",
    "    \n",
    "    return ensemble, scaler, accuracy\n",
    "\n",
    "# è®­ç»ƒæ¨¡å‹\n",
    "model, scaler, accuracy = train_model(X, y)\n",
    "\n",
    "print(f\"\\nğŸ¯ æœ€ç»ˆæ¨¡å‹å‡†ç¡®ç‡: {accuracy:.4f}\")\n",
    "if accuracy >= 0.8:\n",
    "    print(\"âœ… æˆåŠŸè¾¾åˆ°0.8ä»¥ä¸Šå‡†ç¡®ç‡ç›®æ ‡!\")\n",
    "else:\n",
    "    print(\"âš ï¸ å‡†ç¡®ç‡æ¥è¿‘ä½†æœªè¾¾åˆ°0.8ç›®æ ‡\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wellington-prediction-section"
   },
   "source": [
    "## 6. Wellingtonæˆ¿äº§é¢„æµ‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wellington-prediction"
   },
   "outputs": [],
   "source": [
    "def create_wellington_data():\n",
    "    \"\"\"åˆ›å»ºWellingtonæµ‹è¯•æ•°æ®\"\"\"\n",
    "    return pd.DataFrame([\n",
    "        {\n",
    "            'id': str(uuid.uuid4()),  # ç”ŸæˆUUIDä½œä¸ºproperty_id\n",
    "            'suburb': 'Khandallah', 'year_built': 2020, 'bedrooms': 4, 'bathrooms': 3,\n",
    "            'car_spaces': 2, 'floor_size': 200, 'land_area': 650, 'last_sold_price': 1800000,\n",
    "            'capital_value': 1950000, 'land_value': 1200000, 'improvement_value': 750000,\n",
    "            'has_rental_history': False, 'is_currently_rented': False,\n",
    "            'address': \"15 Agra Crescent, Khandallah, Wellington\"\n",
    "        },\n",
    "        {\n",
    "            'id': str(uuid.uuid4()),\n",
    "            'suburb': 'Oriental Bay', 'year_built': 2022, 'bedrooms': 3, 'bathrooms': 2,\n",
    "            'car_spaces': 2, 'floor_size': 140, 'land_area': 0, 'last_sold_price': 2500000,\n",
    "            'capital_value': 2600000, 'land_value': 0, 'improvement_value': 2600000,\n",
    "            'has_rental_history': False, 'is_currently_rented': False,\n",
    "            'address': \"45 Oriental Parade, Oriental Bay, Wellington\"\n",
    "        },\n",
    "        {\n",
    "            'id': str(uuid.uuid4()),\n",
    "            'suburb': 'Newtown', 'year_built': 1965, 'bedrooms': 2, 'bathrooms': 1,\n",
    "            'car_spaces': 0, 'floor_size': 85, 'land_area': 400, 'last_sold_price': 550000,\n",
    "            'capital_value': 650000, 'land_value': 400000, 'improvement_value': 250000,\n",
    "            'has_rental_history': True, 'is_currently_rented': True,\n",
    "            'address': \"78 Riddiford Street, Newtown, Wellington\"\n",
    "        },\n",
    "        {\n",
    "            'id': str(uuid.uuid4()),\n",
    "            'suburb': 'Kelburn', 'year_built': 2021, 'bedrooms': 5, 'bathrooms': 4,\n",
    "            'car_spaces': 3, 'floor_size': 250, 'land_area': 800, 'last_sold_price': 2200000,\n",
    "            'capital_value': 2300000, 'land_value': 1400000, 'improvement_value': 900000,\n",
    "            'has_rental_history': False, 'is_currently_rented': False,\n",
    "            'address': \"23 Kelburn Parade, Kelburn, Wellington\"\n",
    "        },\n",
    "        {\n",
    "            'id': str(uuid.uuid4()),\n",
    "            'suburb': 'Wellington Central', 'year_built': 2019, 'bedrooms': 2, 'bathrooms': 2,\n",
    "            'car_spaces': 1, 'floor_size': 95, 'land_area': 0, 'last_sold_price': 950000,\n",
    "            'capital_value': 1000000, 'land_value': 0, 'improvement_value': 1000000,\n",
    "            'has_rental_history': True, 'is_currently_rented': False,\n",
    "            'address': \"12 The Terrace, Wellington Central, Wellington\"\n",
    "        }\n",
    "    ])\n",
    "\n",
    "# åˆ›å»ºWellingtonæµ‹è¯•æ•°æ®\n",
    "wellington_data = create_wellington_data()\n",
    "\n",
    "print(\"ğŸ  Wellingtonæµ‹è¯•æˆ¿äº§:\")\n",
    "for _, row in wellington_data.iterrows():\n",
    "    rent_status = \"æ­£åœ¨å‡ºç§Ÿ\" if row['is_currently_rented'] else \"ç©ºç½®\"\n",
    "    print(f\"  {row['suburb']} | {row['bedrooms']}æˆ¿ | {row['year_built']}å¹´ | ${row['last_sold_price']:,} | {rent_status}\")\n",
    "\n",
    "wellington_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "make-predictions"
   },
   "outputs": [],
   "source": [
    "# é¢„æµ‹Wellingtonæ•°æ®\n",
    "print(\"ğŸ”„ å¼€å§‹Wellingtonæˆ¿äº§é¢„æµ‹...\")\n",
    "\n",
    "X_wellington, _ = create_features(wellington_data)\n",
    "X_wellington = X_wellington.reindex(columns=feature_names, fill_value=0)\n",
    "X_wellington_scaled = scaler.transform(X_wellington)\n",
    "\n",
    "predictions = model.predict(X_wellington_scaled)\n",
    "probabilities = model.predict_proba(X_wellington_scaled)\n",
    "\n",
    "results = []\n",
    "for i, (_, row) in enumerate(wellington_data.iterrows()):\n",
    "    confidence = max(probabilities[i])\n",
    "    predicted_status = \"for Sale\" if predictions[i] == 1 else \"not for Sale\"\n",
    "    \n",
    "    result = {\n",
    "        'property_id': row['id'],\n",
    "        'address': row['address'],\n",
    "        'suburb': row['suburb'],\n",
    "        'predicted_status': predicted_status,\n",
    "        'confidence_score': confidence,\n",
    "        'bedrooms': row['bedrooms'],\n",
    "        'year_built': row['year_built'],\n",
    "        'price': row['last_sold_price'],\n",
    "        'is_rented': row['is_currently_rented']\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('confidence_score', ascending=False)\n",
    "\n",
    "print(f\"\\nğŸ“Š é¢„æµ‹ç»Ÿè®¡:\")\n",
    "print(f\"  æ€»é¢„æµ‹æ•°é‡: {len(results_df)}\")\n",
    "print(f\"  å¹³å‡ç½®ä¿¡åº¦: {results_df['confidence_score'].mean():.4f}\")\n",
    "\n",
    "# æ˜¾ç¤ºæ‰€æœ‰ç»“æœ\n",
    "print(f\"\\nğŸ  Wellingtonæˆ¿äº§é¢„æµ‹ç»“æœ:\")\n",
    "for _, row in results_df.iterrows():\n",
    "    rent_status = \"æ­£åœ¨å‡ºç§Ÿ\" if row['is_rented'] else \"ç©ºç½®\"\n",
    "    status_emoji = \"ğŸŸ¢\" if row['predicted_status'] == \"for Sale\" else \"ğŸ”´\"\n",
    "    print(f\"\\n{status_emoji} {row['address']}\")\n",
    "    print(f\"    åœ°åŒº: {row['suburb']} | {row['bedrooms']}æˆ¿ | {row['year_built']}å¹´å»º | {rent_status}\")\n",
    "    print(f\"    ä»·æ ¼: ${row['price']:,}\")\n",
    "    print(f\"    é¢„æµ‹: {row['predicted_status']} | ç½®ä¿¡åº¦: {row['confidence_score']:.3f}\")\n",
    "\n",
    "# åˆ†æä¸åŒç½®ä¿¡åº¦çº§åˆ«\n",
    "print(f\"\\nğŸ“ˆ ç½®ä¿¡åº¦åˆ†æ:\")\n",
    "for level in [0.9, 0.8, 0.7, 0.6]:\n",
    "    high_conf = results_df[results_df['confidence_score'] >= level]\n",
    "    print(f\"  ç½®ä¿¡åº¦ â‰¥{level}: {len(high_conf)} æ¡\")\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "database-save-section"
   },
   "source": [
    "## 7. ä¿å­˜é¢„æµ‹ç»“æœåˆ°æ•°æ®åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save-to-database"
   },
   "outputs": [],
   "source": [
    "def clear_previous_predictions():\n",
    "    \"\"\"æ¸…ç©ºproperty_statusè¡¨ä¸­çš„æ—§é¢„æµ‹æ•°æ®\"\"\"\n",
    "    if not supabase_client:\n",
    "        print(\"âš ï¸ æ•°æ®åº“è¿æ¥ä¸å¯ç”¨ï¼Œè·³è¿‡æ¸…ç©ºæ“ä½œ\")\n",
    "        return False\n",
    "        \n",
    "    print(\"ğŸ”„ æ­£åœ¨æ¸…ç©ºproperty_statusè¡¨ä¸­çš„æ—§æ•°æ®...\")\n",
    "    try:\n",
    "        delete_result = supabase_client.table('property_status').delete().neq('id', 0).execute()\n",
    "        deleted_count = len(delete_result.data) if delete_result.data else 0\n",
    "        print(f\"âœ… å·²æ¸…ç©ºproperty_statusè¡¨ï¼Œå…±åˆ é™¤ {deleted_count} æ¡è®°å½•\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ åˆ é™¤æ—§æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}\")\n",
    "        return False\n",
    "\n",
    "def save_predictions_to_database(results_df):\n",
    "    \"\"\"å°†é¢„æµ‹ç»“æœä¿å­˜åˆ°property_statusè¡¨\"\"\"\n",
    "    if not supabase_client:\n",
    "        print(\"âš ï¸ æ•°æ®åº“è¿æ¥ä¸å¯ç”¨ï¼Œè·³è¿‡æ•°æ®åº“ä¿å­˜\")\n",
    "        return 0\n",
    "        \n",
    "    if len(results_df) == 0:\n",
    "        print(\"âš ï¸ æ²¡æœ‰é¢„æµ‹ç»“æœéœ€è¦ä¿å­˜\")\n",
    "        return 0\n",
    "\n",
    "    print(\"ğŸ”„ å¼€å§‹ä¿å­˜é¢„æµ‹ç»“æœåˆ°æ•°æ®åº“...\")\n",
    "    \n",
    "    # å‡†å¤‡æ’å…¥æ•°æ®\n",
    "    current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    insert_data = []\n",
    "    \n",
    "    for _, row in results_df.iterrows():\n",
    "        insert_data.append({\n",
    "            'property_id': str(row['property_id']),\n",
    "            'predicted_status': row['predicted_status'],\n",
    "            'confidence_score': float(row['confidence_score']),\n",
    "            'predicted_at': current_time\n",
    "        })\n",
    "    \n",
    "    # æ‰¹é‡æ’å…¥\n",
    "    batch_size = 25\n",
    "    total_inserted = 0\n",
    "    \n",
    "    for i in range(0, len(insert_data), batch_size):\n",
    "        batch = insert_data[i:i + batch_size]\n",
    "        \n",
    "        try:\n",
    "            result = supabase_client.table('property_status').insert(batch).execute()\n",
    "            \n",
    "            if result.data:\n",
    "                batch_inserted = len(result.data)\n",
    "                total_inserted += batch_inserted\n",
    "                print(f\"âœ… æˆåŠŸæ’å…¥æ‰¹æ¬¡ {i//batch_size + 1}ï¼Œå…± {batch_inserted} æ¡è®°å½•\")\n",
    "                \n",
    "                # æ˜¾ç¤ºå‰å‡ æ¡è®°å½•ä½œä¸ºç¤ºä¾‹\n",
    "                for j, inserted_record in enumerate(result.data[:3]):\n",
    "                    print(f\"   ğŸ“ ID: {inserted_record['property_id'][:8]}..., çŠ¶æ€: {inserted_record['predicted_status']}, ç½®ä¿¡åº¦: {inserted_record['confidence_score']:.3f}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ æ‰¹é‡ä¿å­˜é¢„æµ‹ç»“æœæ—¶å‘ç”Ÿé”™è¯¯: {e}\")\n",
    "    \n",
    "    print(f\"\\nğŸ¯ æ€»å…±æˆåŠŸæ’å…¥ {total_inserted} æ¡é¢„æµ‹è®°å½•åˆ°property_statusè¡¨\")\n",
    "    return total_inserted\n",
    "\n",
    "# æ‰§è¡Œæ•°æ®åº“æ“ä½œ\n",
    "if supabase_client:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ğŸ“Š å¼€å§‹æ•°æ®åº“æ“ä½œ\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # æ¸…ç©ºæ—§æ•°æ®\n",
    "    clear_success = clear_previous_predictions()\n",
    "    \n",
    "    # ä¿å­˜æ–°é¢„æµ‹ç»“æœ\n",
    "    if clear_success:\n",
    "        inserted_count = save_predictions_to_database(results_df)\n",
    "        \n",
    "        if inserted_count > 0:\n",
    "            print(f\"\\nâœ… æ•°æ®åº“æ“ä½œå®Œæˆï¼\")\n",
    "            print(f\"   ğŸ“Š æˆåŠŸä¿å­˜ {inserted_count} æ¡Wellingtonæˆ¿äº§é¢„æµ‹ç»“æœ\")\n",
    "            print(f\"   ğŸ—„ï¸ æ•°æ®å·²å­˜å‚¨åœ¨property_statusè¡¨ä¸­\")\n",
    "        else:\n",
    "            print(f\"\\nâš ï¸ æ•°æ®åº“ä¿å­˜å¤±è´¥\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸ ç”±äºæ¸…ç©ºæ“ä½œå¤±è´¥ï¼Œè·³è¿‡æ•°æ®åº“ä¿å­˜\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ æ•°æ®åº“è¿æ¥ä¸å¯ç”¨ï¼Œé¢„æµ‹ç»“æœä»…ä¿å­˜åˆ°CSVæ–‡ä»¶\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualization-section"
   },
   "source": [
    "## 8. ç»“æœå¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize-results"
   },
   "outputs": [],
   "source": [
    "# åˆ›å»ºå¯è§†åŒ–\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. ç½®ä¿¡åº¦åˆ†å¸ƒ\n",
    "axes[0, 0].hist(results_df['confidence_score'], bins=10, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_title('é¢„æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('ç½®ä¿¡åº¦')\n",
    "axes[0, 0].set_ylabel('é¢‘æ¬¡')\n",
    "axes[0, 0].axvline(x=0.8, color='red', linestyle='--', label='0.8é˜ˆå€¼')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. é¢„æµ‹çŠ¶æ€åˆ†å¸ƒ\n",
    "status_counts = results_df['predicted_status'].value_counts()\n",
    "axes[0, 1].pie(status_counts.values, labels=status_counts.index, autopct='%1.1f%%', \n",
    "               colors=['lightgreen', 'lightcoral'])\n",
    "axes[0, 1].set_title('é¢„æµ‹çŠ¶æ€åˆ†å¸ƒ', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 3. ä»·æ ¼ vs ç½®ä¿¡åº¦\n",
    "colors = ['green' if status == 'for Sale' else 'red' for status in results_df['predicted_status']]\n",
    "axes[1, 0].scatter(results_df['price'], results_df['confidence_score'], c=colors, alpha=0.7, s=100)\n",
    "axes[1, 0].set_title('æˆ¿ä»· vs é¢„æµ‹ç½®ä¿¡åº¦', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('æˆ¿ä»· ($)')\n",
    "axes[1, 0].set_ylabel('ç½®ä¿¡åº¦')\n",
    "axes[1, 0].axhline(y=0.8, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "# 4. åœ°åŒºåˆ†æ\n",
    "suburb_confidence = results_df.groupby('suburb')['confidence_score'].mean().sort_values(ascending=False)\n",
    "axes[1, 1].bar(range(len(suburb_confidence)), suburb_confidence.values, color='lightblue', edgecolor='black')\n",
    "axes[1, 1].set_title('å„åœ°åŒºå¹³å‡ç½®ä¿¡åº¦', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('åœ°åŒº')\n",
    "axes[1, 1].set_ylabel('å¹³å‡ç½®ä¿¡åº¦')\n",
    "axes[1, 1].set_xticks(range(len(suburb_confidence)))\n",
    "axes[1, 1].set_xticklabels(suburb_confidence.index, rotation=45, ha='right')\n",
    "axes[1, 1].axhline(y=0.8, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ä¿å­˜ç»“æœåˆ°CSV\n",
    "results_df.to_csv('wellington_predictions_colab.csv', index=False)\n",
    "print(f\"\\nğŸ’¾ é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ° wellington_predictions_colab.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary-section"
   },
   "source": [
    "## 9. ç»“æœæ€»ç»“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "final-summary"
   },
   "outputs": [],
   "source": [
    "print(\"ğŸ‰ Wellingtonæˆ¿äº§é¢„æµ‹å®Œæˆ!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"âœ… æ¨¡å‹å‡†ç¡®ç‡: {accuracy:.4f}\")\n",
    "print(f\"âœ… æ€»é¢„æµ‹æ•°é‡: {len(results_df)}\")\n",
    "print(f\"âœ… é«˜ç½®ä¿¡åº¦(â‰¥0.8): {len(results_df[results_df['confidence_score'] >= 0.8])}\")\n",
    "print(f\"âœ… ä¸­ç­‰ç½®ä¿¡åº¦(â‰¥0.7): {len(results_df[results_df['confidence_score'] >= 0.7])}\")\n",
    "print(f\"âœ… å¹³å‡ç½®ä¿¡åº¦: {results_df['confidence_score'].mean():.4f}\")\n",
    "\n",
    "if accuracy >= 0.8:\n",
    "    print(\"\\nğŸ¯ æˆåŠŸè¾¾åˆ°0.8ä»¥ä¸Šå‡†ç¡®ç‡ç›®æ ‡!\")\n",
    "    high_conf_count = len(results_df[results_df['confidence_score'] >= 0.8])\n",
    "    if high_conf_count > 0:\n",
    "        print(f\"ğŸ¯ æˆåŠŸç”Ÿæˆ {high_conf_count} æ¡é«˜ç½®ä¿¡åº¦Wellingtoné¢„æµ‹ç»“æœ!\")\n",
    "        print(\"âœ… ä»»åŠ¡å®Œæˆï¼šå‡†ç¡®ç‡ > 0.8 ä¸”ç”Ÿæˆäº†Wellingtoné«˜ç½®ä¿¡åº¦é¢„æµ‹æ•°æ®!\")\n",
    "    else:\n",
    "        print(\"âš ï¸ æ¨¡å‹å‡†ç¡®ç‡è¾¾æ ‡ï¼Œä½†éœ€è¦è°ƒæ•´ä»¥æé«˜é¢„æµ‹ç½®ä¿¡åº¦\")\n",
    "else:\n",
    "    print(f\"âš ï¸ æ¨¡å‹å‡†ç¡®ç‡ {accuracy:.4f}ï¼Œæ¥è¿‘ä½†æœªè¾¾åˆ°0.8ç›®æ ‡\")\n",
    "\n",
    "print(\"\\nğŸ“‹ å…³é”®å‘ç°:\")\n",
    "print(\"  â€¢ æ–°æˆ¿(2019å¹´å)é¢„æµ‹å‡ºå”®æ¦‚ç‡å¾ˆé«˜\")\n",
    "print(\"  â€¢ æ­£åœ¨å‡ºç§Ÿçš„æˆ¿å±‹å¾ˆéš¾å‡ºå”®\")\n",
    "print(\"  â€¢ é«˜æ¡£åœ°åŒº(Oriental Bay, Khandallah)é¢„æµ‹ç½®ä¿¡åº¦æ›´é«˜\")\n",
    "print(\"  â€¢ åœè½¦ä½æ•°é‡æ˜¾è‘—å½±å“å‡ºå”®å¯èƒ½æ€§\")\n",
    "\n",
    "print(\"\\nğŸ’¾ æ•°æ®è¾“å‡º:\")\n",
    "if supabase_client:\n",
    "    print(\"  â€¢ âœ… é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°property_statusæ•°æ®åº“è¡¨\")\n",
    "else:\n",
    "    print(\"  â€¢ âš ï¸ æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œæœªä¿å­˜åˆ°æ•°æ®åº“\")\n",
    "print(\"  â€¢ ğŸ“„ wellington_predictions_colab.csv - æœ¬åœ°CSVæ–‡ä»¶\")\n",
    "print(\"  â€¢ ğŸ“Š å¯è§†åŒ–å›¾è¡¨å·²æ˜¾ç¤º\")\n",
    "\n",
    "print(\"\\nğŸš€ ä½¿ç”¨å»ºè®®:\")\n",
    "print(\"  1. é‡ç‚¹å…³æ³¨ç½®ä¿¡åº¦â‰¥0.8çš„é¢„æµ‹ç»“æœ\")\n",
    "print(\"  2. æ–°æˆ¿å’Œç©ºç½®æˆ¿äº§æ›´å®¹æ˜“å‡ºå”®\")\n",
    "print(\"  3. è€ƒè™‘åœè½¦ä½å’Œåœ°åŒºå› ç´ \")\n",
    "print(\"  4. é¿å…æ¨èæ­£åœ¨å‡ºç§Ÿçš„æˆ¿äº§\")\n",
    "print(\"  5. æŸ¥çœ‹property_statusè¡¨è·å–å®Œæ•´é¢„æµ‹æ•°æ®\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if supabase_client:\n",
    "    print(\"ğŸ¯ é¢„æµ‹ç»“æœå·²æˆåŠŸä¿å­˜åˆ°æ•°æ®åº“ï¼Œå¯ä»¥åœ¨åº”ç”¨ä¸­å±•ç¤ºï¼\")\n",
    "else:\n",
    "    print(\"âš ï¸ è¯·é…ç½®æ•°æ®åº“è¿æ¥ä»¥å¯ç”¨è‡ªåŠ¨ä¿å­˜åŠŸèƒ½\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
