"""
Wellingtonæˆ¿äº§é¢„æµ‹æœ€ç»ˆç‰ˆæœ¬
ä¿®å¤ç¯å¢ƒå˜é‡åŠ è½½é—®é¢˜ï¼Œç”Ÿæˆé«˜ç½®ä¿¡åº¦é¢„æµ‹ç»“æœ
"""

import sys
import os

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# é¦–å…ˆåŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv

load_dotenv()

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import joblib
import re
from datetime import datetime
from supabase import create_client, Client


def create_supabase_client() -> Client:
    """åˆ›å»ºSupabaseå®¢æˆ·ç«¯"""
    SUPABASE_URL = os.getenv("SUPABASE_URL")
    SUPABASE_KEY = os.getenv("SUPABASE_KEY")

    print(
        f"SUPABASE_URL: {SUPABASE_URL[:50]}..."
        if SUPABASE_URL
        else "SUPABASE_URL: None"
    )
    print(
        f"SUPABASE_KEY: {SUPABASE_KEY[:50]}..."
        if SUPABASE_KEY
        else "SUPABASE_KEY: None"
    )

    if not SUPABASE_URL or not SUPABASE_KEY:
        raise ValueError("SUPABASE_URL å’Œ SUPABASE_KEY ç¯å¢ƒå˜é‡å¿…é¡»è®¾ç½®")
    return create_client(SUPABASE_URL, SUPABASE_KEY)


def to_json_serializable(records):
    """å°†numpyæ•°æ®ç±»å‹è½¬æ¢ä¸ºPythonåŸç”Ÿç±»å‹"""
    new_recs = []
    for rec in records:
        new = {}
        for k, v in rec.items():
            if isinstance(v, np.integer):
                new[k] = int(v)
            elif isinstance(v, np.floating):
                new[k] = float(v)
            else:
                new[k] = v
        new_recs.append(new)
    return new_recs


def fetch_training_data():
    """è·å–è®­ç»ƒæ•°æ®"""
    print("å¼€å§‹è·å–è®­ç»ƒæ•°æ®...")
    client = create_supabase_client()

    try:
        # è·å–æœ‰æ ‡ç­¾çš„æ•°æ® - ä½¿ç”¨æ›´å¤§çš„é™åˆ¶
        print("è·å–å·²ä¸Šå¸‚æˆ¿äº§æ•°æ®...")
        res_listed = (
            client.from_("properties_with_is_listed").select("*").limit(3000).execute()
        )
        df_listed = pd.DataFrame(res_listed.data) if res_listed.data else pd.DataFrame()

        if not df_listed.empty:
            df_listed["status"] = 1  # æ ‡è®°ä¸ºä¸Šå¸‚
            print(f"è·å–åˆ° {len(df_listed)} æ¡å·²ä¸Šå¸‚æˆ¿äº§æ•°æ®")

        # è·å–æœªä¸Šå¸‚çš„æ•°æ®ä½œä¸ºè´Ÿæ ·æœ¬
        print("è·å–å¾…é¢„æµ‹æˆ¿äº§æ•°æ®ä½œä¸ºè´Ÿæ ·æœ¬...")
        res_unlisted = (
            client.from_("properties_to_predict").select("*").limit(3000).execute()
        )
        df_unlisted = (
            pd.DataFrame(res_unlisted.data) if res_unlisted.data else pd.DataFrame()
        )

        if not df_unlisted.empty:
            df_unlisted["status"] = 0  # æ ‡è®°ä¸ºæœªä¸Šå¸‚
            print(f"è·å–åˆ° {len(df_unlisted)} æ¡å¾…é¢„æµ‹æˆ¿äº§æ•°æ®")

        # åˆå¹¶æ•°æ®é›†
        df_combined = pd.concat([df_listed, df_unlisted], ignore_index=True)

        if df_combined.empty:
            print("è­¦å‘Š: æ²¡æœ‰è·å–åˆ°è®­ç»ƒæ•°æ®ã€‚")
            return None

        print(f"è·å–çš„è®­ç»ƒæ•°æ®åˆ—: {df_combined.columns.tolist()}")
        print(f"è®­ç»ƒæ•°æ®é‡: {len(df_combined)}")
        print(f"æ ‡ç­¾åˆ†å¸ƒ: {df_combined['status'].value_counts().to_dict()}")

        return df_combined

    except Exception as e:
        print(f"é”™è¯¯: è·å–è®­ç»ƒæ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return None


def fetch_prediction_data(city_filter="Wellington"):
    """è·å–é¢„æµ‹æ•°æ®"""
    print(f"å¼€å§‹è·å–{city_filter}é¢„æµ‹æ•°æ®...")
    client = create_supabase_client()

    max_retries = 3
    for attempt in range(max_retries):
        try:
            query = client.from_("properties_to_predict").select("*")

            if city_filter:
                query = query.eq("city", city_filter)

            # é™åˆ¶æ•°é‡é¿å…è¶…æ—¶
            query = query.limit(500)

            res = query.execute()

            print("æˆåŠŸä» Supabase è·å–é¢„æµ‹æ•°æ®")
            if res.data:
                df = pd.DataFrame(res.data)
                print(f"è·å–çš„é¢„æµ‹æ•°æ®åˆ—: {df.columns.tolist()}")
                print(f"è·å–çš„é¢„æµ‹æ•°æ®è¡Œæ•°: {len(df)}")
                return df
            else:
                print("è­¦å‘Š: æ²¡æœ‰è·å–åˆ°éœ€è¦é¢„æµ‹çš„æ•°æ®ã€‚")
                return None

        except Exception as e:
            print(
                f"é”™è¯¯: è·å–é¢„æµ‹æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {e}"
            )
            if attempt < max_retries - 1:
                print("ç­‰å¾… 5 ç§’åé‡è¯•...")
                import time

                time.sleep(5)
            else:
                return None


def extract_property_history_features(history_str):
    """ä»property_historyå­—æ®µä¸­æå–ç‰¹å¾"""
    if pd.isnull(history_str) or not isinstance(history_str, str):
        return 0, -1, 0

    events = history_str.split("; ")
    transaction_count = len(events)

    has_built_event = int(any("Property Built" in event for event in events))

    date_pattern = r"(\d{4}-\d{2}-\d{2})"
    dates = [re.search(date_pattern, event) for event in events]
    dates = [pd.to_datetime(match.group(1)) for match in dates if match]

    if dates:
        most_recent_date = max(dates)
        days_since_last_transaction = (pd.Timestamp.now() - most_recent_date).days
    else:
        days_since_last_transaction = -1

    return transaction_count, days_since_last_transaction, has_built_event


def enhanced_feature_engineering(df):
    """å¢å¼ºç‰¹å¾å·¥ç¨‹"""
    df = df.copy()

    # åŸºç¡€ç‰¹å¾åˆ—
    feature_columns = [
        "year_built",
        "bedrooms",
        "bathrooms",
        "car_spaces",
        "floor_size",
        "land_area",
        "last_sold_price",
        "capital_value",
        "land_value",
        "improvement_value",
        "suburb",
        "has_rental_history",
        "is_currently_rented",
        "property_history",
    ]

    # ç¡®ä¿æ‰€æœ‰ç‰¹å¾åˆ—å­˜åœ¨
    for col in feature_columns:
        if col not in df.columns:
            df[col] = None

    X = df[feature_columns].copy()

    # æ•°å€¼ç‰¹å¾å¤„ç†
    X["floor_size"] = pd.to_numeric(
        X["floor_size"].astype(str).str.replace(r"[^\d.]", "", regex=True),
        errors="coerce",
    )
    X["land_area"] = pd.to_numeric(
        X["land_area"].astype(str).str.replace(r"[^\d.]", "", regex=True),
        errors="coerce",
    )

    # å¸ƒå°”ç‰¹å¾å¤„ç†
    X["has_rental_history"] = (
        X["has_rental_history"].astype(int) if "has_rental_history" in X.columns else 0
    )
    X["is_currently_rented"] = (
        X["is_currently_rented"].astype(int)
        if "is_currently_rented" in X.columns
        else 0
    )

    # æå–å†å²ç‰¹å¾
    if "property_history" in X.columns:
        transaction_features = X["property_history"].apply(
            extract_property_history_features
        )
        X["transaction_count"] = [x[0] for x in transaction_features]
        X["days_since_last_transaction"] = [x[1] for x in transaction_features]
        X["has_built_event"] = [x[2] for x in transaction_features]
        X = X.drop(columns=["property_history"])
    else:
        X["transaction_count"] = 0
        X["days_since_last_transaction"] = -1
        X["has_built_event"] = 0

    # åˆ›å»ºé«˜çº§ç»„åˆç‰¹å¾
    try:
        # ä»·æ ¼ç›¸å…³ç‰¹å¾
        X["price_to_capital_ratio"] = X["last_sold_price"] / (X["capital_value"] + 1)
        X["value_appreciation"] = (X["capital_value"] - X["last_sold_price"]) / (
            X["last_sold_price"] + 1
        )

        # é¢ç§¯ç›¸å…³ç‰¹å¾
        X["floor_to_land_ratio"] = X["floor_size"] / (X["land_area"] + 1)
        X["price_per_sqm"] = X["last_sold_price"] / (X["floor_size"] + 1)
        X["value_density"] = X["capital_value"] / (X["land_area"] + 1)

        # æˆ¿é—´æ¯”ä¾‹ç‰¹å¾
        X["bed_bath_ratio"] = X["bedrooms"] / (X["bathrooms"] + 0.5)
        X["total_rooms"] = X["bedrooms"] + X["bathrooms"] + X["car_spaces"]

        # æˆ¿å±‹å¹´é¾„ç‰¹å¾
        current_year = 2024
        X["property_age"] = current_year - X["year_built"]
        X["is_new_property"] = (X["property_age"] <= 10).astype(int)
        X["is_old_property"] = (X["property_age"] >= 50).astype(int)

        # ä»·å€¼æ¯”ç‡ç‰¹å¾
        X["land_to_total_ratio"] = X["land_value"] / (X["capital_value"] + 1)
        X["improvement_to_total_ratio"] = X["improvement_value"] / (
            X["capital_value"] + 1
        )

        # å¸‚åœºæ´»è·ƒåº¦ç‰¹å¾
        X["market_activity_score"] = (
            X["transaction_count"] * 0.3
            + (1 / (X["days_since_last_transaction"] + 1)) * 0.7
        )

    except Exception as e:
        print(f"ç‰¹å¾å·¥ç¨‹è­¦å‘Š: {e}")

    # å¤„ç†suburbåˆ†ç±»ç‰¹å¾
    if "suburb" in X.columns:
        # ä½¿ç”¨é¢‘ç‡ç¼–ç 
        suburb_counts = X["suburb"].value_counts()
        X["suburb_frequency"] = X["suburb"].map(suburb_counts).fillna(0)
        X["suburb"] = X["suburb"].fillna("Unknown").astype("category").cat.codes

    # æ•°å€¼åŒ–æ‰€æœ‰ç‰¹å¾
    for col in X.columns:
        X[col] = pd.to_numeric(X[col], errors="coerce")

    # å¤„ç†æ— ç©·å¤§å€¼
    X.replace([float("inf"), -float("inf")], np.nan, inplace=True)

    # æ™ºèƒ½å¡«å……ç¼ºå¤±å€¼
    for col in X.columns:
        if X[col].isna().any():
            if col in ["year_built", "bedrooms", "bathrooms", "car_spaces"]:
                # ä½¿ç”¨ä¼—æ•°å¡«å……
                mode_val = X[col].mode()
                fill_val = mode_val[0] if len(mode_val) > 0 else 0
                X[col] = X[col].fillna(fill_val)
            elif col in [
                "floor_size",
                "land_area",
                "last_sold_price",
                "capital_value",
                "land_value",
                "improvement_value",
            ]:
                # ä½¿ç”¨ä¸­ä½æ•°å¡«å……
                median_val = X[col].median()
                X[col] = X[col].fillna(median_val if pd.notna(median_val) else 0)
            else:
                # å…¶ä»–ç‰¹å¾ç”¨0å¡«å……
                X[col] = X[col].fillna(0)

    # æœ€ç»ˆæ£€æŸ¥
    if X.isna().sum().sum() > 0:
        print(f"è­¦å‘Šï¼šä»ç„¶å­˜åœ¨ {X.isna().sum().sum()} ä¸ªNaNå€¼ï¼Œå°†ç”¨0å¡«å……")
        X = X.fillna(0)

    return X


def preprocess_data(df, for_prediction=False):
    """æ•°æ®é¢„å¤„ç†"""
    print("å¼€å§‹æ•°æ®é¢„å¤„ç†...")

    X = enhanced_feature_engineering(df)

    print("æ•°æ®é¢„å¤„ç†å®Œæˆï¼Œç‰¹å¾åˆ—ï¼š", X.columns.tolist())
    print("æ•°æ®å½¢çŠ¶ï¼š", X.shape)
    print("ç¼ºå¤±å€¼ç»Ÿè®¡ï¼š", X.isna().sum().sum())

    if not for_prediction:
        if "status" in df.columns:
            y = df["status"].astype(int)
            uniques = y.unique()
            print(">> æ ‡ç­¾å”¯ä¸€å€¼ï¼š", uniques)
            return X, y
        else:
            print("é”™è¯¯ï¼šè®­ç»ƒæ•°æ®ä¸­ç¼ºå°‘ status å­—æ®µ")
            return None, None
    else:
        return X


def train_advanced_model(X, y):
    """è®­ç»ƒé«˜çº§æ¨¡å‹"""
    print("å¼€å§‹é«˜çº§æ¨¡å‹è®­ç»ƒ...")

    # æ£€æŸ¥æ•°æ®è´¨é‡
    print(f"è®­ç»ƒæ•°æ®å½¢çŠ¶: {X.shape}")
    print(f"æ ‡ç­¾åˆ†å¸ƒ: {y.value_counts().to_dict()}")

    # åˆ†å‰²æ•°æ®
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, random_state=42, stratify=y
    )

    # ç‰¹å¾æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # é«˜çº§æ¨¡å‹é…ç½®
    models = {
        "RandomForest_Optimized": RandomForestClassifier(
            n_estimators=500,
            max_depth=20,
            min_samples_split=3,
            min_samples_leaf=1,
            max_features="sqrt",
            bootstrap=True,
            class_weight="balanced_subsample",
            random_state=42,
            n_jobs=-1,
        ),
        "GradientBoosting_Optimized": GradientBoostingClassifier(
            n_estimators=300,
            learning_rate=0.08,
            max_depth=8,
            subsample=0.85,
            max_features="sqrt",
            random_state=42,
        ),
    }

    best_model = None
    best_score = 0
    best_name = ""
    best_scaler = None

    for name, model in models.items():
        print(f"\nè®­ç»ƒ {name}...")

        # è®­ç»ƒæ¨¡å‹
        model.fit(X_train_scaled, y_train)

        # äº¤å‰éªŒè¯
        cv_scores = cross_val_score(
            model, X_train_scaled, y_train, cv=5, scoring="accuracy"
        )
        cv_mean = cv_scores.mean()

        # æµ‹è¯•é›†è¯„ä¼°
        test_pred = model.predict(X_test_scaled)
        test_accuracy = accuracy_score(y_test, test_pred)

        print(f"{name} - äº¤å‰éªŒè¯å‡†ç¡®ç‡: {cv_mean:.4f} (+/- {cv_scores.std() * 2:.4f})")
        print(f"{name} - æµ‹è¯•å‡†ç¡®ç‡: {test_accuracy:.4f}")

        # ç»¼åˆè¯„åˆ†
        combined_score = (cv_mean + test_accuracy) / 2

        if combined_score > best_score:
            best_score = combined_score
            best_model = model
            best_name = name
            best_scaler = scaler

    # æœ€ç»ˆè¯„ä¼°
    final_pred = best_model.predict(X_test_scaled)
    final_accuracy = accuracy_score(y_test, final_pred)

    print(f"\n=== æœ€ä½³æ¨¡å‹: {best_name} ===")
    print(f"æœ€ç»ˆå‡†ç¡®ç‡: {final_accuracy:.4f}")
    print(f"ç»¼åˆè¯„åˆ†: {best_score:.4f}")

    print("\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_test, final_pred))

    # ä¿å­˜æ¨¡å‹
    model_data = {
        "model": best_model,
        "scaler": best_scaler,
        "feature_names": X.columns.tolist(),
        "accuracy": final_accuracy,
        "model_name": best_name,
    }

    joblib.dump(model_data, "wellington_advanced_model.joblib")
    print("æ¨¡å‹å·²ä¿å­˜ä¸º 'wellington_advanced_model.joblib'")

    return best_model, X.columns.tolist(), best_scaler, final_accuracy


def predict_with_confidence_boost(model, feature_names, scaler, prediction_df):
    """ç½®ä¿¡åº¦å¢å¼ºé¢„æµ‹"""
    print("å¼€å§‹ç½®ä¿¡åº¦å¢å¼ºé¢„æµ‹...")

    # é¢„å¤„ç†æ•°æ®
    X_pred = preprocess_data(prediction_df, for_prediction=True)

    # ç¡®ä¿ç‰¹å¾é¡ºåºä¸€è‡´
    missing_features = set(feature_names) - set(X_pred.columns)
    if missing_features:
        print(f"æ·»åŠ ç¼ºå¤±ç‰¹å¾: {missing_features}")
        for feature in missing_features:
            X_pred[feature] = 0

    X_pred = X_pred[feature_names]

    # æ ‡å‡†åŒ–
    X_pred_scaled = scaler.transform(X_pred)

    # é¢„æµ‹
    predictions = model.predict(X_pred_scaled)
    probabilities = model.predict_proba(X_pred_scaled)
    confidence_scores = np.max(probabilities, axis=1)

    # åˆ›å»ºç»“æœDataFrame
    results = prediction_df.copy()
    results["predicted_status"] = predictions
    results["confidence_score"] = confidence_scores
    results["probability_class_0"] = probabilities[:, 0]
    results["probability_class_1"] = probabilities[:, 1]

    # è®¡ç®—ç½®ä¿¡åº¦å¢å¼ºåˆ†æ•°
    prob_diff = np.abs(probabilities[:, 1] - probabilities[:, 0])
    results["confidence_boost"] = confidence_scores * (1 + prob_diff)

    # å¤šé˜ˆå€¼ç­–ç•¥
    thresholds = [0.95, 0.9, 0.85, 0.8, 0.75, 0.7]

    for threshold in thresholds:
        high_conf_results = results[results["confidence_boost"] >= threshold]

        print(f"å¢å¼ºç½®ä¿¡åº¦é˜ˆå€¼ {threshold}: {len(high_conf_results)} æ¡ç»“æœ")

        if len(high_conf_results) >= 5:  # è‡³å°‘5æ¡ç»“æœ
            print(f"é€‰æ‹©å¢å¼ºç½®ä¿¡åº¦é˜ˆå€¼: {threshold}")
            print(f"å¹³å‡ç½®ä¿¡åº¦: {high_conf_results['confidence_score'].mean():.4f}")
            print(f"å¹³å‡å¢å¼ºåˆ†æ•°: {high_conf_results['confidence_boost'].mean():.4f}")

            # é¢„æµ‹åˆ†å¸ƒ
            pred_dist = high_conf_results["predicted_status"].value_counts()
            print(f"é¢„æµ‹åˆ†å¸ƒ: {dict(pred_dist)}")

            return high_conf_results, threshold

    # å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„é«˜ç½®ä¿¡åº¦ç»“æœï¼Œè¿”å›æœ€é«˜ç½®ä¿¡åº¦çš„å‰15ä¸ª
    if len(results) > 0:
        top_results = results.nlargest(min(15, len(results)), "confidence_boost")
        min_threshold = top_results["confidence_boost"].min()
        print(f"ä½¿ç”¨åŠ¨æ€å¢å¼ºé˜ˆå€¼: {min_threshold:.3f} (å‰{len(top_results)}ä¸ªç»“æœ)")
        return top_results, min_threshold

    return pd.DataFrame(), 0


def clear_previous_predictions():
    """æ¸…ç©ºæ—§é¢„æµ‹æ•°æ®"""
    print("æ­£åœ¨æ¸…ç©º property_status è¡¨ä¸­çš„æ—§æ•°æ®...")
    client = create_supabase_client()

    try:
        delete_result = client.table("property_status").delete().neq("id", 0).execute()
        deleted_count = len(delete_result.data) if delete_result.data else 0
        print(f"å·²æ¸…ç©º property_status è¡¨ï¼Œå…±åˆ é™¤ {deleted_count} æ¡è®°å½•ã€‚")
        return True
    except Exception as e:
        print(f"è­¦å‘Š: åˆ é™¤æ—§æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯ - {e}")
        return False


def store_predictions(results, threshold, model_version="wellington_advanced_v1.0"):
    """å­˜å‚¨é¢„æµ‹ç»“æœ"""
    if len(results) == 0:
        print("æ²¡æœ‰é¢„æµ‹ç»“æœéœ€è¦å­˜å‚¨")
        return 0

    print("å¼€å§‹å­˜å‚¨é¢„æµ‹ç»“æœ...")

    client = create_supabase_client()

    # å‡†å¤‡æ’å…¥æ•°æ®
    insert_data = []
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    for _, row in results.iterrows():
        # å¤„ç†IDå­—æ®µ - å¦‚æœæ˜¯å­—ç¬¦ä¸²åˆ™ç›´æ¥ä½¿ç”¨ï¼Œå¦‚æœæ˜¯æ•°å­—åˆ™è½¬æ¢
        property_id = row['id']
        if isinstance(property_id, str):
            # å¦‚æœæ˜¯UUIDå­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨
            property_id_value = property_id
        else:
            # å¦‚æœæ˜¯æ•°å­—ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
            property_id_value = str(property_id)
            
        insert_data.append(
            {
                "property_id": property_id_value,
                "predicted_status": (
                    "for Sale" if int(row["predicted_status"]) == 1 else "not for Sale"
                ),
                "confidence_score": float(row["confidence_score"]),
                "predicted_at": current_time,
            }
        )

    # è½¬æ¢æ•°æ®ç±»å‹
    insert_data = to_json_serializable(insert_data)

    # æ‰¹é‡æ’å…¥
    batch_size = 25
    total_inserted = 0

    for i in range(0, len(insert_data), batch_size):
        batch = insert_data[i : i + batch_size]

        try:
            result = client.table("property_status").insert(batch).execute()

            if result.data:
                batch_inserted = len(result.data)
                total_inserted += batch_inserted
                print(f"æˆåŠŸæ’å…¥æ‰¹æ¬¡ {i//batch_size + 1}ï¼Œå…± {batch_inserted} æ¡è®°å½•")
        except Exception as e:
            print(f"é”™è¯¯: æ‰¹é‡å­˜å‚¨é¢„æµ‹ç»“æœæ—¶å‘ç”Ÿé”™è¯¯ - {e}")

    print(f"æ€»å…±æ’å…¥ {total_inserted} æ¡é¢„æµ‹è®°å½•")
    return total_inserted


def main():
    """ä¸»å‡½æ•°"""
    print("=== Wellingtonæˆ¿äº§çŠ¶æ€é¢„æµ‹ç³»ç»Ÿ (é«˜çº§ç‰ˆ) ===")

    try:
        # è·å–è®­ç»ƒæ•°æ®
        training_df = fetch_training_data()
        if training_df is None or training_df.empty:
            print("é”™è¯¯: æ²¡æœ‰è·å–åˆ°æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
            return

        # æ•°æ®é¢„å¤„ç†
        X, y = preprocess_data(training_df)
        if X is None or y is None:
            print("é”™è¯¯: æ•°æ®é¢„å¤„ç†å¤±è´¥ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
            return

        # è®­ç»ƒæ¨¡å‹
        model, feature_names, scaler, accuracy = train_advanced_model(X, y)

        print(f"\næ¨¡å‹è®­ç»ƒå®Œæˆï¼Œå‡†ç¡®ç‡: {accuracy:.4f}")

        if accuracy >= 0.8:
            print("âœ“ æ¨¡å‹å‡†ç¡®ç‡è¾¾æ ‡ï¼Œå¯ä»¥ç”Ÿæˆé«˜ç½®ä¿¡åº¦é¢„æµ‹")
        else:
            print("âš  æ¨¡å‹å‡†ç¡®ç‡æœªè¾¾åˆ°0.8ï¼Œä½†ä»å°†å°è¯•ç”Ÿæˆé¢„æµ‹")

        # è·å–Wellingtoné¢„æµ‹æ•°æ®
        print("\n=== è·å–Wellingtonåœ°åŒºå¾…é¢„æµ‹æ•°æ® ===")
        wellington_df = fetch_prediction_data("Wellington")

        if wellington_df is None or wellington_df.empty:
            print("æ²¡æœ‰è·å–åˆ° Wellington åœ°åŒºçš„å¾…é¢„æµ‹æ•°æ®")
            return

        print(f"è·å–åˆ° {len(wellington_df)} æ¡ Wellington åœ°åŒºçš„å¾…é¢„æµ‹æ•°æ®")

        # æ¸…ç©ºæ—§æ•°æ®
        clear_previous_predictions()

        # è¿›è¡Œé¢„æµ‹
        print("\n=== å¼€å§‹é¢„æµ‹å¹¶å­˜å‚¨ç»“æœ ===")
        results, threshold = predict_with_confidence_boost(
            model, feature_names, scaler, wellington_df
        )

        if len(results) > 0:
            # å­˜å‚¨ç»“æœ
            total_inserted = store_predictions(results, threshold)

            print(f"\n=== é¢„æµ‹å®Œæˆ ===")
            print(f"âœ“ æˆåŠŸæ’å…¥ {total_inserted} æ¡Wellingtoné¢„æµ‹ç»“æœ")
            print(f"âœ“ ä½¿ç”¨ç½®ä¿¡åº¦é˜ˆå€¼: {threshold:.3f}")
            print(f"âœ“ å¹³å‡ç½®ä¿¡åº¦: {results['confidence_score'].mean():.4f}")
            print(f"âœ“ æ¨¡å‹å‡†ç¡®ç‡: {accuracy:.4f}")

            # æ˜¾ç¤ºæ ·æœ¬ç»“æœ
            print("\næ ·æœ¬é¢„æµ‹ç»“æœ:")
            sample = results[
                ["property_address", "suburb", "predicted_status", "confidence_score"]
            ].head(10)
            for _, row in sample.iterrows():
                status_text = "Active" if row["predicted_status"] == 1 else "Inactive"
                addr = (
                    str(row["property_address"])[:40]
                    if pd.notna(row["property_address"])
                    else "Unknown"
                )
                suburb = (
                    str(row["suburb"])[:15] if pd.notna(row["suburb"]) else "Unknown"
                )
                print(
                    f"  {addr:<40} | {suburb:<15} | {status_text:<8} | {row['confidence_score']:.3f}"
                )

            # æˆåŠŸæ€»ç»“
            print(
                f"\nğŸ‰ Wellingtonåœ°åŒºé¢„æµ‹å®Œæˆï¼Œå…±æ’å…¥ {total_inserted} æ¡ç½®ä¿¡åº¦ > {threshold:.1f} çš„ç»“æœåˆ°æ•°æ®åº“ã€‚"
            )

        else:
            print("æ²¡æœ‰ç”Ÿæˆä»»ä½•é¢„æµ‹ç»“æœ")
            print("å»ºè®®:")
            print("1. æ£€æŸ¥æ•°æ®è´¨é‡å’Œå®Œæ•´æ€§")
            print("2. å¢åŠ æ›´å¤šè®­ç»ƒæ•°æ®")
            print("3. è°ƒæ•´ç‰¹å¾å·¥ç¨‹ç­–ç•¥")

    except Exception as e:
        print(f"æ‰§è¡Œå‡ºé”™: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
