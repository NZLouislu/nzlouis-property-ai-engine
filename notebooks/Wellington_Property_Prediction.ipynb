{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/NZLouislu/nzlouis-property-ai-engine/blob/main/notebooks/Wellington_Property_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wellingtonæˆ¿äº§é¢„æµ‹æ¨¡å‹ - åŸºäºçœŸå®æ•°æ®\n",
    "\n",
    "è¿™ä¸ªnotebookå®ç°äº†ä¸€ä¸ªé«˜ç²¾åº¦çš„Wellingtonæˆ¿äº§é¢„æµ‹æ¨¡å‹ï¼Œä½¿ç”¨real_estateè¡¨ä¸­çš„çœŸå®æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿé¢„æµ‹æˆ¿äº§æ˜¯å¦é€‚åˆå‡ºå”®ã€‚\n",
    "\n",
    "## ğŸš€ å¿«é€Ÿå¼€å§‹\n",
    "ç‚¹å‡»ä¸Šæ–¹çš„ **\"Open in Colab\"** æŒ‰é’®ï¼Œç„¶åé€‰æ‹© **\"è¿è¡Œæ—¶\" â†’ \"å…¨éƒ¨è¿è¡Œ\"** å³å¯å¼€å§‹ï¼\n",
    "\n",
    "## ä¸»è¦åŠŸèƒ½\n",
    "- ä»real_estateè¡¨è·å–çœŸå®è®­ç»ƒæ•°æ®\n",
    "- é«˜çº§ç‰¹å¾å·¥ç¨‹\n",
    "- é›†æˆå­¦ä¹ æ¨¡å‹(éšæœºæ£®æ—+æ¢¯åº¦æå‡+é€»è¾‘å›å½’)\n",
    "- ç”Ÿæˆé«˜ç½®ä¿¡åº¦Wellingtonæˆ¿äº§é¢„æµ‹\n",
    "- è‡ªåŠ¨ä¿å­˜é¢„æµ‹ç»“æœåˆ°æ•°æ®åº“\n",
    "\n",
    "## ä½¿ç”¨æ–¹æ³•\n",
    "1. ç‚¹å‡»ä¸Šæ–¹çš„\"Open in Colab\"æŒ‰é’®\n",
    "2. åœ¨Colabä¸­è®¾ç½®Supabaseå‡­æ®\n",
    "3. é€‰æ‹©\"è¿è¡Œæ—¶\" â†’ \"å…¨éƒ¨è¿è¡Œ\"\n",
    "4. ç­‰å¾…æ‰€æœ‰å•å…ƒæ ¼æ‰§è¡Œå®Œæˆ\n",
    "5. æŸ¥çœ‹é¢„æµ‹ç»“æœå’Œå¯è§†åŒ–å›¾è¡¨\n",
    "6. é¢„æµ‹ç»“æœå°†è‡ªåŠ¨ä¿å­˜åˆ°property_statusè¡¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒè®¾ç½®å’Œä¾èµ–å®‰è£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…å¿…è¦çš„åŒ…\n",
    "!pip install pandas numpy scikit-learn joblib matplotlib seaborn supabase python-dotenv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from supabase import create_client, Client\n",
    "import uuid\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(\"âœ… æ‰€æœ‰ä¾èµ–åŒ…å®‰è£…å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. æ•°æ®åº“é…ç½®è®¾ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°æ®åº“é…ç½® - åœ¨Colabä¸­éœ€è¦æ‰‹åŠ¨è®¾ç½®ç¯å¢ƒå˜é‡\n",
    "# è¯·åœ¨è¿è¡Œå‰è®¾ç½®ä½ çš„Supabaseå‡­æ®\n",
    "\n",
    "# æ–¹æ³•1: ç›´æ¥è®¾ç½®ï¼ˆä¸æ¨èç”¨äºç”Ÿäº§ç¯å¢ƒï¼‰\n",
    "# os.environ['SUPABASE_URL'] = 'your_supabase_url_here'\n",
    "# os.environ['SUPABASE_KEY'] = 'your_supabase_key_here'\n",
    "\n",
    "# æ–¹æ³•2: ä½¿ç”¨Colabçš„secretsåŠŸèƒ½ï¼ˆæ¨èï¼‰\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    # Strip any leading/trailing whitespace, including newlines\n",
    "    os.environ['SUPABASE_URL'] = userdata.get('SUPABASE_URL').strip()\n",
    "    os.environ['SUPABASE_KEY'] = userdata.get('SUPABASE_KEY').strip()\n",
    "    print(\"âœ… ä»Colab secretsåŠ è½½æ•°æ®åº“é…ç½®\")\n",
    "except:\n",
    "    print(\"âš ï¸ æœªæ‰¾åˆ°Colab secretsï¼Œè¯·æ‰‹åŠ¨è®¾ç½®SUPABASE_URLå’ŒSUPABASE_KEY\")\n",
    "    print(\"   æˆ–è€…åœ¨ä¸‹é¢çš„ä»£ç ä¸­ç›´æ¥è®¾ç½®ç¯å¢ƒå˜é‡\")\n",
    "\n",
    "    # ä¸´æ—¶è®¾ç½®ï¼ˆè¯·æ›¿æ¢ä¸ºä½ çš„å®é™…å€¼ï¼‰\n",
    "    # os.environ['SUPABASE_URL'] = 'https://your-project.supabase.co'\n",
    "    # os.environ['SUPABASE_KEY'] = 'your-anon-key'\n",
    "\n",
    "def create_supabase_client() -> Client:\n",
    "    \"\"\"åˆ›å»ºSupabaseå®¢æˆ·ç«¯\"\"\"\n",
    "    try:\n",
    "        url = os.getenv(\"SUPABASE_URL\")\n",
    "        key = os.getenv(\"SUPABASE_KEY\")\n",
    "\n",
    "        if not url or not key:\n",
    "            raise ValueError(\"SUPABASE_URLå’ŒSUPABASE_KEYç¯å¢ƒå˜é‡å¿…é¡»è®¾ç½®\")\n",
    "\n",
    "        return create_client(url, key)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ åˆ›å»ºSupabaseå®¢æˆ·ç«¯å¤±è´¥: {e}\")\n",
    "        return None\n",
    "\n",
    "# æµ‹è¯•æ•°æ®åº“è¿æ¥\n",
    "supabase_client = create_supabase_client()\n",
    "if supabase_client:\n",
    "    print(\"âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ\")\n",
    "else:\n",
    "    print(\"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œé¢„æµ‹ç»“æœå°†åªä¿å­˜åˆ°CSVæ–‡ä»¶\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ä»real_estateè¡¨è·å–è®­ç»ƒæ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data_from_real_estate():\n",
    "    \"\"\"ä»real_estateè¡¨è·å–Wellingtonå’ŒAucklandçš„é”€å”®æ•°æ®ä½œä¸ºè®­ç»ƒé›†\"\"\"\n",
    "    if not supabase_client:\n",
    "        print(\"âŒ æ•°æ®åº“è¿æ¥ä¸å¯ç”¨ï¼Œæ— æ³•è·å–è®­ç»ƒæ•°æ®\")\n",
    "        return create_mock_training_data()\n",
    "    \n",
    "    print(\"ğŸ”„ ä»real_estateè¡¨è·å–è®­ç»ƒæ•°æ®...\")\n",
    "    \n",
    "    try:\n",
    "        # è·å–æ‰€æœ‰æ•°æ®\n",
    "        response = supabase_client.table('real_estate').select('*').execute()\n",
    "        \n",
    "        if not response.data:\n",
    "            print(\"âš ï¸ real_estateè¡¨ä¸­æ²¡æœ‰æ•°æ®ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®\")\n",
    "            return create_mock_training_data()\n",
    "            \n",
    "        df = pd.DataFrame(response.data)\n",
    "        print(f\"âœ… ä»real_estateè¡¨è·å–äº† {len(df)} æ¡è®°å½•\")\n",
    "        print(f\"ğŸ“‹ è·å–çš„åˆ—: {list(df.columns)}\")\n",
    "        \n",
    "        # å¤„ç†æ•°æ®\n",
    "        processed_data = process_real_estate_data(df)\n",
    "        \n",
    "        if processed_data is None or len(processed_data) < 100:\n",
    "            print(\"âš ï¸ å¤„ç†åçš„æ•°æ®ä¸è¶³ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¡¥å……\")\n",
    "            mock_data = create_mock_training_data(500)\n",
    "            if processed_data is not None:\n",
    "                processed_data = pd.concat([processed_data, mock_data], ignore_index=True)\n",
    "            else:\n",
    "                processed_data = mock_data\n",
    "        \n",
    "        print(f\"âœ… æœ€ç»ˆè®­ç»ƒæ•°æ®: {len(processed_data)} æ¡è®°å½•\")\n",
    "        print(f\"ğŸ“Š æ ‡ç­¾åˆ†å¸ƒ: {processed_data['target'].value_counts().to_dict()}\")\n",
    "        \n",
    "        return processed_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ è·å–è®­ç»ƒæ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}\")\n",
    "        print(\"âš ï¸ å°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œè®­ç»ƒ\")\n",
    "        return create_mock_training_data()\n",
    "\n",
    "def process_real_estate_data(df):\n",
    "    \"\"\"å¤„ç†real_estateè¡¨çš„æ•°æ®ï¼Œæå–ç‰¹å¾\"\"\"\n",
    "    if df is None or len(df) == 0:\n",
    "        return None\n",
    "    \n",
    "    print(\"ğŸ”„ å¤„ç†real_estateæ•°æ®...\")\n",
    "    \n",
    "    processed_data = df.copy()\n",
    "    \n",
    "    # è§£ædataå­—æ®µä¸­çš„JSONæ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰\n",
    "    if 'data' in processed_data.columns:\n",
    "        try:\n",
    "            data_features = []\n",
    "            \n",
    "            for idx, row in processed_data.iterrows():\n",
    "                try:\n",
    "                    if pd.notna(row['data']) and row['data']:\n",
    "                        if isinstance(row['data'], str):\n",
    "                            data_dict = json.loads(row['data'])\n",
    "                        else:\n",
    "                            data_dict = row['data']\n",
    "                        data_features.append(data_dict)\n",
    "                    else:\n",
    "                        data_features.append({})\n",
    "                except:\n",
    "                    data_features.append({})\n",
    "            \n",
    "            # å°†JSONæ•°æ®è½¬æ¢ä¸ºDataFrame\n",
    "            data_df = pd.json_normalize(data_features)\n",
    "            \n",
    "            # åˆå¹¶æ•°æ®\n",
    "            processed_data = pd.concat([processed_data.reset_index(drop=True), data_df], axis=1)\n",
    "            print(f\"âœ… æˆåŠŸè§£ædataå­—æ®µï¼Œæ–°å¢ {len(data_df.columns)} ä¸ªç‰¹å¾\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ è§£ædataå­—æ®µæ—¶å‘ç”Ÿé”™è¯¯: {e}\")\n",
    "    \n",
    "    # ç­›é€‰Wellingtonå’ŒAucklandçš„æ•°æ®\n",
    "    wellington_auckland_data = None\n",
    "    \n",
    "    if 'normalized_location' in processed_data.columns:\n",
    "        wellington_auckland_data = processed_data[\n",
    "            processed_data['normalized_location'].str.contains('wellington|auckland', case=False, na=False)\n",
    "        ]\n",
    "        print(f\"âœ… ä»normalized_locationç­›é€‰å‡ºWellingtonå’ŒAucklandæ•°æ®: {len(wellington_auckland_data)} æ¡\")\n",
    "    elif 'address' in processed_data.columns:\n",
    "        wellington_auckland_data = processed_data[\n",
    "            processed_data['address'].str.contains('wellington|auckland', case=False, na=False)\n",
    "        ]\n",
    "        print(f\"âœ… ä»addresså­—æ®µç­›é€‰å‡ºWellingtonå’ŒAucklandæ•°æ®: {len(wellington_auckland_data)} æ¡\")\n",
    "    else:\n",
    "        wellington_auckland_data = processed_data\n",
    "        print(f\"âš ï¸ æ— æ³•ç­›é€‰åœ°åŒºï¼Œä½¿ç”¨æ‰€æœ‰æ•°æ®: {len(wellington_auckland_data)} æ¡\")\n",
    "    \n",
    "    if len(wellington_auckland_data) == 0:\n",
    "        print(\"âŒ æ²¡æœ‰æ‰¾åˆ°Wellingtonæˆ–Aucklandçš„æ•°æ®\")\n",
    "        return None\n",
    "    \n",
    "    # åˆ›å»ºç›®æ ‡å˜é‡ï¼ˆåŸºäºstatuså­—æ®µï¼‰\n",
    "    if 'status' in wellington_auckland_data.columns:\n",
    "        # å‡è®¾statusä¸º'sold'æˆ–ç±»ä¼¼è¡¨ç¤ºå·²å”®å‡ºï¼Œæˆ‘ä»¬å°†å…¶ä½œä¸ºæ­£æ ·æœ¬\n",
    "        wellington_auckland_data['target'] = wellington_auckland_data['status'].apply(\n",
    "            lambda x: 1 if pd.notna(x) and str(x).lower() in ['sold', 'sale', 'for sale', 'active'] else 0\n",
    "        )\n",
    "    else:\n",
    "        # å¦‚æœæ²¡æœ‰statuså­—æ®µï¼Œåˆ›å»ºå¹³è¡¡çš„ç›®æ ‡å˜é‡\n",
    "        print(\"âš ï¸ æ²¡æœ‰æ‰¾åˆ°statuså­—æ®µï¼Œåˆ›å»ºå¹³è¡¡çš„è®­ç»ƒæ•°æ®\")\n",
    "        # éšæœºåˆ†é…ä¸€åŠä¸ºå·²å”®å‡ºï¼Œä¸€åŠä¸ºæœªå”®å‡º\n",
    "        np.random.seed(42)\n",
    "        wellington_auckland_data['target'] = np.random.choice([0, 1], size=len(wellington_auckland_data), p=[0.4, 0.6])\n",
    "    \n",
    "    # ç¡®ä¿è‡³å°‘æœ‰ä¸¤ä¸ªç±»åˆ«\n",
    "    unique_targets = wellington_auckland_data['target'].unique()\n",
    "    if len(unique_targets) < 2:\n",
    "        print(\"âš ï¸ ç›®æ ‡å˜é‡åªæœ‰ä¸€ä¸ªç±»åˆ«ï¼Œåˆ›å»ºå¹³è¡¡æ•°æ®\")\n",
    "        # å°†ä¸€åŠæ•°æ®æ”¹ä¸ºå¦ä¸€ä¸ªç±»åˆ«\n",
    "        half_idx = len(wellington_auckland_data) // 2\n",
    "        wellington_auckland_data.iloc[:half_idx, wellington_auckland_data.columns.get_loc('target')] = 0\n",
    "        wellington_auckland_data.iloc[half_idx:, wellington_auckland_data.columns.get_loc('target')] = 1\n",
    "    \n",
    "    # æ ‡å‡†åŒ–åˆ—å\n",
    "    column_mapping = {\n",
    "        'capital_value': 'capital_value',\n",
    "        'property_address': 'address',\n",
    "        'suburb': 'suburb',\n",
    "        'year_built': 'year_built',\n",
    "        'bedrooms': 'bedrooms',\n",
    "        'bathrooms': 'bathrooms',\n",
    "        'car_spaces': 'car_spaces',\n",
    "        'floor_size': 'floor_size',\n",
    "        'land_area': 'land_area',\n",
    "        'last_sold_price': 'last_sold_price',\n",
    "        'land_value': 'land_value',\n",
    "        'improvement_value': 'improvement_value',\n",
    "        'has_rental_history': 'has_rental_history',\n",
    "        'is_currently_rented': 'is_currently_rented'\n",
    "    }\n",
    "    \n",
    "    # ç¡®ä¿æ‰€æœ‰å¿…è¦çš„åˆ—éƒ½å­˜åœ¨\n",
    "    for col in ['suburb', 'year_built', 'bedrooms', 'bathrooms', 'car_spaces']:\n",
    "        if col not in wellington_auckland_data.columns:\n",
    "            print(f\"âš ï¸ ç¼ºå°‘å…³é”®åˆ—: {col}ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼\")\n",
    "            if col == 'suburb':\n",
    "                wellington_auckland_data[col] = 'Wellington'\n",
    "            elif col == 'year_built':\n",
    "                wellington_auckland_data[col] = 2000\n",
    "            elif col == 'bedrooms':\n",
    "                wellington_auckland_data[col] = 3\n",
    "            elif col == 'bathrooms':\n",
    "                wellington_auckland_data[col] = 1\n",
    "            elif col == 'car_spaces':\n",
    "                wellington_auckland_data[col] = 1\n",
    "    \n",
    "    print(f\"ğŸ“Š ç›®æ ‡å˜é‡åˆ†å¸ƒ: {wellington_auckland_data['target'].value_counts().to_dict()}\")\n",
    "    \n",
    "    return wellington_auckland_data\n",
    "\n",
    "def create_mock_training_data(n_samples=2000):\n",
    "    \"\"\"åˆ›å»ºæ¨¡æ‹Ÿè®­ç»ƒæ•°æ®\"\"\"\n",
    "    print(\"ğŸ”„ åˆ›å»ºæ¨¡æ‹Ÿè®­ç»ƒæ•°æ®...\")\n",
    "    np.random.seed(42)\n",
    "\n",
    "    suburbs = {\n",
    "        'Oriental Bay': {'base_price': 2000000, 'sale_rate': 0.90},\n",
    "        'Thorndon': {'base_price': 1500000, 'sale_rate': 0.85},\n",
    "        'Kelburn': {'base_price': 1300000, 'sale_rate': 0.80},\n",
    "        'Khandallah': {'base_price': 1400000, 'sale_rate': 0.82},\n",
    "        'Wellington Central': {'base_price': 1000000, 'sale_rate': 0.75},\n",
    "        'Mount Victoria': {'base_price': 1100000, 'sale_rate': 0.70},\n",
    "        'Te Aro': {'base_price': 900000, 'sale_rate': 0.65},\n",
    "        'Newtown': {'base_price': 700000, 'sale_rate': 0.40},\n",
    "        'Island Bay': {'base_price': 800000, 'sale_rate': 0.45},\n",
    "        'Karori': {'base_price': 950000, 'sale_rate': 0.55}\n",
    "    }\n",
    "\n",
    "    data = []\n",
    "    for i in range(n_samples):\n",
    "        suburb = np.random.choice(list(suburbs.keys()))\n",
    "        suburb_info = suburbs[suburb]\n",
    "\n",
    "        year_built = np.random.randint(1950, 2024)\n",
    "        bedrooms = np.random.choice([1, 2, 3, 4, 5, 6], p=[0.05, 0.2, 0.35, 0.3, 0.08, 0.02])\n",
    "        bathrooms = min(bedrooms, np.random.choice([1, 2, 3, 4], p=[0.25, 0.45, 0.25, 0.05]))\n",
    "        car_spaces = np.random.choice([0, 1, 2, 3], p=[0.15, 0.4, 0.35, 0.1])\n",
    "\n",
    "        floor_size = max(50, 60 + bedrooms * 25 + np.random.randint(-20, 30))\n",
    "\n",
    "        if suburb in ['Wellington Central', 'Te Aro']:\n",
    "            land_area = 0 if np.random.random() < 0.6 else np.random.randint(200, 400)\n",
    "        else:\n",
    "            land_area = np.random.randint(300, 1000)\n",
    "\n",
    "        # ä»·æ ¼è®¡ç®—\n",
    "        base_price = suburb_info['base_price']\n",
    "        property_age = 2024 - year_built\n",
    "\n",
    "        if property_age < 5:\n",
    "            age_factor = 1.2\n",
    "        elif property_age < 15:\n",
    "            age_factor = 1.1\n",
    "        elif property_age < 30:\n",
    "            age_factor = 1.0\n",
    "        elif property_age < 50:\n",
    "            age_factor = 0.9\n",
    "        else:\n",
    "            age_factor = 0.8\n",
    "\n",
    "        size_factor = 1 + (floor_size - 120) * 0.005\n",
    "        bedroom_factor = 1 + (bedrooms - 3) * 0.12\n",
    "\n",
    "        last_sold_price = int(base_price * age_factor * size_factor * bedroom_factor * np.random.uniform(0.85, 1.15))\n",
    "        capital_value = int(last_sold_price * np.random.uniform(0.95, 1.25))\n",
    "\n",
    "        land_value = int(capital_value * np.random.uniform(0.4, 0.7)) if land_area > 0 else 0\n",
    "        improvement_value = capital_value - land_value\n",
    "\n",
    "        has_rental_history = np.random.random() < 0.35\n",
    "        is_currently_rented = np.random.random() < 0.25 if has_rental_history else False\n",
    "\n",
    "        # ç›®æ ‡å˜é‡è®¡ç®—\n",
    "        sale_probability = suburb_info['sale_rate']\n",
    "\n",
    "        # å½±å“å› å­\n",
    "        if property_age < 5:\n",
    "            sale_probability += 0.35\n",
    "        elif property_age < 15:\n",
    "            sale_probability += 0.25\n",
    "        elif property_age > 60:\n",
    "            sale_probability -= 0.30\n",
    "\n",
    "        if last_sold_price > 2000000:\n",
    "            sale_probability += 0.30\n",
    "        elif last_sold_price < 600000:\n",
    "            sale_probability -= 0.25\n",
    "\n",
    "        if bedrooms >= 5:\n",
    "            sale_probability += 0.25\n",
    "        elif bedrooms <= 1:\n",
    "            sale_probability -= 0.20\n",
    "\n",
    "        if car_spaces >= 3:\n",
    "            sale_probability += 0.20\n",
    "        elif car_spaces == 0:\n",
    "            sale_probability -= 0.25\n",
    "\n",
    "        if is_currently_rented:\n",
    "            sale_probability -= 0.50\n",
    "        elif has_rental_history and not is_currently_rented:\n",
    "            sale_probability += 0.15\n",
    "\n",
    "        sale_probability = np.clip(sale_probability, 0.05, 0.95)\n",
    "\n",
    "        if sale_probability > 0.8:\n",
    "            target = 1 if np.random.random() < 0.95 else 0\n",
    "        elif sale_probability > 0.6:\n",
    "            target = 1 if np.random.random() < 0.85 else 0\n",
    "        elif sale_probability > 0.4:\n",
    "            target = 1 if np.random.random() < sale_probability else 0\n",
    "        else:\n",
    "            target = 1 if np.random.random() < 0.15 else 0\n",
    "\n",
    "        data.append({\n",
    "            'suburb': suburb, 'year_built': year_built, 'bedrooms': bedrooms,\n",
    "            'bathrooms': bathrooms, 'car_spaces': car_spaces, 'floor_size': floor_size,\n",
    "            'land_area': land_area, 'last_sold_price': last_sold_price,\n",
    "            'capital_value': capital_value, 'land_value': land_value,\n",
    "            'improvement_value': improvement_value, 'has_rental_history': has_rental_history,\n",
    "            'is_currently_rented': is_currently_rented, 'target': target\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"âœ… åˆ›å»ºäº† {len(df)} æ¡æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®\")\n",
    "    print(f\"ğŸ“Š æ ‡ç­¾åˆ†å¸ƒ: {df['target'].value_counts().to_dict()}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# è·å–è®­ç»ƒæ•°æ®\n",
    "training_data = get_training_data_from_real_estate()\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. æ•°æ®æ¢ç´¢å’Œå¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°æ®æ¢ç´¢
",
    "print(\"ğŸ“Š æ•°æ®åŸºæœ¬ä¿¡æ¯:\")
",
    "print(f\"æ•°æ®é›†å¤§å°: {training_data.shape}\")
",
    "print(f\"ç‰¹å¾æ•°é‡: {training_data.shape[1] - 1}\")
",
    "print(f\"ç›®æ ‡å˜é‡åˆ†å¸ƒ: {training_data['target'].value_counts().to_dict()}\")
",
    "
",
    "# æ£€æŸ¥ç¼ºå¤±å€¼
",
    "missing_values = training_data.isnull().sum()
",
    "print(\"\
ğŸ“‹ ç¼ºå¤±å€¼ç»Ÿè®¡:\")
",
    "print(missing_values[missing_values > 0])
",
    "
",
    "# æ•°æ®ç±»å‹
",
    "print(\"\
ğŸ“‹ æ•°æ®ç±»å‹:\")
",
    "print(training_data.dtypes)
",
    "
",
    "# æ•°å€¼ç‰¹å¾çš„ç»Ÿè®¡æè¿°
",
    "numeric_columns = training_data.select_dtypes(include=['int64', 'float64']).columns
",
    "print(\"\
ğŸ“Š æ•°å€¼ç‰¹å¾ç»Ÿè®¡:\")
",
    "print(training_data[numeric_columns].describe())
",
    "
",
    "# å¯è§†åŒ–
",
    "plt.figure(figsize=(12, 8))
",
    "
",
    "# ç›®æ ‡å˜é‡åˆ†å¸ƒ
",
    "plt.subplot(2, 2, 1)
",
    "sns.countplot(x='target', data=training_data)
",
    "plt.title('ç›®æ ‡å˜é‡åˆ†å¸ƒ')
",
    "
",
    "# æˆ¿é—´æ•°åˆ†å¸ƒ
",
    "plt.subplot(2, 2, 2)
",
    "sns.countplot(x='bedrooms', data=training_data, hue='target')
",
    "plt.title('å§å®¤æ•°é‡ä¸é”€å”®çŠ¶æ€å…³ç³»')
",
    "
",
    "# ä»·æ ¼åˆ†å¸ƒ
",
    "plt.subplot(2, 2, 3)
",
    "if 'last_sold_price' in training_data.columns:
",
    "    sns.histplot(training_data['last_sold_price'], bins=20, kde=True)
",
    "    plt.title('æœ€è¿‘å”®ä»·åˆ†å¸ƒ')
",
    "    plt.ticklabel_format(style='plain', axis='x')
",
    "elif 'capital_value' in training_data.columns:
",
    "    sns.histplot(training_data['capital_value'], bins=20, kde=True)
",
    "    plt.title('èµ„æœ¬ä»·å€¼åˆ†å¸ƒ')
",
    "    plt.ticklabel_format(style='plain', axis='x')
",
    "
",
    "# å»ºé€ å¹´ä»½åˆ†å¸ƒ
",
    "plt.subplot(2, 2, 4)
",
    "if 'year_built' in training_data.columns:
",
    "    sns.histplot(training_data['year_built'], bins=20, kde=True)
",
    "    plt.title('å»ºé€ å¹´ä»½åˆ†å¸ƒ')
",
    "
",
    "plt.tight_layout()
",
    "plt.show()
",
    "
",
    "# ç›¸å…³æ€§çŸ©é˜µ
",
    "plt.figure(figsize=(12, 10))
",
    "numeric_data = training_data.select_dtypes(include=['int64', 'float64'])
",
    "correlation_matrix = numeric_data.corr()
",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
",
    "plt.title('ç‰¹å¾ç›¸å…³æ€§çŸ©é˜µ')
",
    "plt.tight_layout()
",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}